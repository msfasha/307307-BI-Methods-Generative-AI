{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaf4065",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/20243-NLP-LLM/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446113f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20243-NLP-LLM/Part%202%20-%20Introduction%20to%20NNs%20and%20Word%20Embeddings/2-neural_network_from_scratch.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f18101",
   "metadata": {},
   "source": [
    "## Creating a Neural Network to Predict/Solve XOR Binary Gate using NumPy\n",
    "\n",
    "This notebook demonstrates the implementation of a simple feedforward neural network using NumPy to solve the classic XOR problem. The XOR function is not linearly separable, making it a great case study for neural networks with hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XORNeuralNetwork:\n",
    "    def __init__(self, input_size=2, hidden_size=2, output_size=1, learning_rate=0.1, random_seed=42):\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - np.power(np.tanh(x), 2)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa10f50",
   "metadata": {},
   "source": [
    "### Forward and Backward Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    self.z1 = np.dot(X, self.W1) + self.b1\n",
    "    self.a1 = self.tanh(self.z1)\n",
    "    self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "    self.a2 = self.sigmoid(self.z2)\n",
    "    return self.a2\n",
    "\n",
    "def backward(self, X, y, output):\n",
    "    error = y.reshape(output.shape) - output\n",
    "    delta2 = error * self.sigmoid_derivative(self.z2)\n",
    "    delta1 = np.dot(delta2, self.W2.T) * self.tanh_derivative(self.z1)\n",
    "    dW2 = np.dot(self.a1.T, delta2)\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "    self.W1 += self.learning_rate * dW1\n",
    "    self.b1 += self.learning_rate * db1\n",
    "    self.W2 += self.learning_rate * dW2\n",
    "    self.b2 += self.learning_rate * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f6988",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, y, epochs=10000):\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        output = self.forward(X)\n",
    "        self.backward(X, y, output)\n",
    "        loss = np.mean(np.square(y - output))\n",
    "        losses.append(loss)\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.10f}\")\n",
    "    return losses\n",
    "\n",
    "def predict(self, X):\n",
    "    predictions = self.forward(X)\n",
    "    return np.round(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcdc9b",
   "metadata": {},
   "source": [
    "### Visualizing the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5029b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(self):\n",
    "    print(\"\\nWeights (input to hidden) W1:\")\n",
    "    print(self.W1)\n",
    "    print(\"\\nBiases (hidden) b1:\")\n",
    "    print(self.b1)\n",
    "    print(\"\\nWeights (hidden to output) W2:\")\n",
    "    print(self.W2)\n",
    "    print(\"\\nBiases (output) b2:\")\n",
    "    print(self.b2)\n",
    "\n",
    "def visualize_forward_pass(self, X):\n",
    "    print(\"\\n--- Forward Pass Visualization ---\")\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    for i, x in enumerate(X):\n",
    "        x = x.reshape(1, -1)\n",
    "        print(f\"\\nInput {i+1}: {x.flatten()}\")\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.tanh(z1)\n",
    "        print(\"Z1 = X · W1 + b1\")\n",
    "        print(f\"Z1 = {z1.flatten()}\")\n",
    "        print(f\"A1 = tanh(Z1) = {a1.flatten()}\")\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "        print(\"Z2 = A1 · W2 + b2\")\n",
    "        print(f\"Z2 = {z2.flatten()}\")\n",
    "        print(f\"A2 = sigmoid(Z2) = {a2.flatten()}\")\n",
    "        print(f\"Prediction: {np.round(a2).flatten()[0]}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7bfc5",
   "metadata": {},
   "source": [
    "### Training the Neural Network on XOR Binary Gate\n",
    "\n",
    "This section trains the neural network on the XOR dataset and displays the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Create and train the network\n",
    "nn = XORNeuralNetwork()\n",
    "print(\"Initial weights:\")\n",
    "nn.print_weights()\n",
    "print(\"\\nTraining the neural network...\")\n",
    "losses = nn.train(X, y)\n",
    "print(\"\\nTraining complete\")\n",
    "nn.print_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24908d1",
   "metadata": {},
   "source": [
    "## Evaluating the Model and Visualizing Results\n",
    "\n",
    "After training, we evaluate the model on the XOR inputs and visualize the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = nn.predict(X)\n",
    "print(\"\\nPredictions:\")\n",
    "for x_i, pred, target in zip(X, predictions, y):\n",
    "    print(f\"Input: {x_i}, Prediction: {pred[0]}, Target: {target}\")\n",
    "accuracy = np.mean(predictions.flatten() == y)\n",
    "print(f\"\\nAccuracy: {accuracy * 100}%\")\n",
    "\n",
    "# Visualize forward pass\n",
    "nn.visualize_forward_pass(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv310)",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
