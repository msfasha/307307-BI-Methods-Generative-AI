{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/20243-NLP-LLM/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20243-NLP-LLM/Part%203%20-%20Introduction%20to%20DL%20and%20LLMs/1-Introduction%20to%20DL%20and%20LLMs.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da4bf6",
   "metadata": {},
   "source": [
    "### Context Aware Word Embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd402358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2e2e8",
   "metadata": {},
   "source": [
    "### Display BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d483e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'bank':\n",
      "tensor([ 4.7019e-01, -1.9835e-01, -1.0122e-01, -1.3519e-01,  1.2612e+00,\n",
      "        -9.6139e-03, -4.9014e-02,  1.0147e+00, -4.5361e-02,  1.7432e-01,\n",
      "         1.2800e-01, -3.2356e-01, -1.3227e-01,  3.6582e-02, -7.8302e-01,\n",
      "        -6.2770e-01,  5.2776e-01,  3.5693e-01,  1.3597e+00,  2.3784e-01,\n",
      "        -3.0995e-01,  4.3136e-02,  3.2358e-01,  3.2144e-01,  3.3207e-01,\n",
      "         4.5470e-01,  6.8660e-01,  5.2037e-01, -2.8076e-01, -5.2107e-01,\n",
      "         5.3412e-01,  9.5313e-01,  3.6960e-01,  4.9074e-01,  1.0348e-01,\n",
      "        -1.2543e-01,  1.8115e-01,  3.9604e-02, -1.1310e+00,  2.2161e-02,\n",
      "        -4.4877e-01, -8.1382e-01, -6.2421e-01,  3.5284e-01, -2.4929e-01,\n",
      "        -6.1539e-01,  1.9276e-01,  2.8171e-01, -7.0082e-01, -8.2422e-01,\n",
      "        -3.0416e-01,  1.0278e+00,  4.3732e-01, -5.0054e-01,  1.1097e-01,\n",
      "         4.7545e-01, -1.0476e+00, -4.6538e-01, -5.3300e-01, -2.1977e-01,\n",
      "         7.0954e-01,  3.1443e-01,  5.0420e-01, -7.7659e-01,  2.3119e-01,\n",
      "        -1.6568e-01,  4.8205e-01, -5.2181e-03, -1.1804e-01, -9.9157e-02,\n",
      "        -2.4582e-01,  1.9177e+00, -8.5401e-01, -9.5023e-01, -4.6143e-01,\n",
      "         5.0577e-01, -2.2459e-01,  2.9306e-01, -2.2287e-01, -9.2279e-03,\n",
      "        -5.4351e-01, -7.2676e-01, -1.5370e-01,  3.8075e-01,  3.8973e-01,\n",
      "        -6.8711e-01,  1.1629e-01,  1.9948e-01, -4.7043e-01,  1.2130e+00,\n",
      "        -1.4237e-01,  2.6370e-01, -9.1200e-01, -2.1185e-01, -1.3493e+00,\n",
      "        -2.0093e-02, -8.5553e-02,  7.4500e-02, -2.7861e-01,  5.7108e-01,\n",
      "         6.0045e-01, -1.1580e+00,  2.2457e-01,  5.2492e-02,  7.7377e-02,\n",
      "         2.1038e-01, -1.0968e+00, -1.8806e-01,  1.6193e-04,  3.0690e-01,\n",
      "         1.5269e-01, -1.2186e+00,  4.4192e-01, -8.6209e-01, -3.1611e-01,\n",
      "        -5.5617e-01,  2.4480e-02, -6.4080e-01, -8.4688e-01, -3.2271e-01,\n",
      "         5.6128e-01,  6.9319e-01,  3.6391e-01,  1.4688e+00,  3.0880e-01,\n",
      "         3.2401e-03,  5.1110e-02,  9.8396e-01, -5.8512e-01, -1.1058e-01,\n",
      "         5.6782e-01,  5.1704e-01,  4.8607e-02,  3.8416e-02,  1.0723e-01,\n",
      "        -5.8880e-01, -3.2970e-03, -4.8399e-01, -1.0776e+00,  3.2018e-01,\n",
      "         8.1521e-01,  8.5122e-01,  5.0063e-01,  1.1447e+00, -1.8171e-01,\n",
      "         4.7930e-01,  6.8499e-02, -9.1607e-01, -2.6628e-01, -1.0907e-01,\n",
      "         1.1996e-01, -3.1764e-01, -3.9093e-01, -5.3363e-01,  2.1021e-01,\n",
      "        -1.9328e-01, -4.6874e-01,  1.2553e-01, -5.8689e-01,  1.6500e-02,\n",
      "         2.6675e-01,  4.8762e-03, -4.1349e-01, -1.0107e-01, -5.9281e-01,\n",
      "        -3.6083e-01,  9.8077e-02,  4.3204e-01, -3.1348e-01,  2.6281e-01,\n",
      "        -4.4020e-01,  9.3242e-02,  1.1335e+00, -3.1242e-01,  9.5865e-01,\n",
      "         4.3642e-01, -9.1704e-01, -5.5271e-01,  5.3453e-01,  8.2077e-01,\n",
      "        -3.4234e-02,  9.8388e-01,  5.4295e-01,  6.9819e-01,  1.0401e+00,\n",
      "         5.3207e-01,  1.7786e-01,  1.1376e-01,  1.5848e-01, -2.3289e-01,\n",
      "        -4.0887e-01, -4.7423e-01, -3.7506e-01,  3.0031e-02, -3.0034e-01,\n",
      "        -3.0897e-01, -3.0131e-01, -2.1677e-01, -8.5815e-01, -3.3625e-01,\n",
      "         1.7334e-01, -1.0664e+00,  2.9628e-01,  1.5440e-01, -8.2941e-02,\n",
      "         1.9345e-01,  4.2606e-01,  3.3704e-01,  1.4883e+00, -1.4554e-01,\n",
      "        -7.5085e-01,  5.5163e-01, -9.2985e-01,  1.2159e-01, -2.5228e-01,\n",
      "         4.6588e-01, -1.2934e-02, -8.3803e-01,  3.8504e-02,  4.2223e-01,\n",
      "         1.9769e-01, -4.9039e-02,  7.5554e-02,  7.1144e-01,  1.0876e-01,\n",
      "         6.0351e-01,  3.1833e-01, -9.8136e-02,  8.2948e-02, -1.1635e-01,\n",
      "         1.1131e-01,  1.5795e-01,  1.0441e+00, -7.2768e-02, -2.5971e-01,\n",
      "        -4.1198e-01, -1.6680e+00, -7.8462e-01, -4.9429e-01, -2.1480e-02,\n",
      "        -4.3348e-01, -3.0725e-01, -2.0169e-01, -3.1562e-01,  5.5264e-01,\n",
      "         8.7739e-01, -2.5813e-01, -4.6994e-01,  2.2665e-01, -6.8825e-01,\n",
      "        -1.0124e-01, -8.5311e-01, -4.8196e-01, -8.7491e-01,  8.3805e-01,\n",
      "        -2.4941e-01, -4.3674e-02,  1.3940e-02, -7.0811e-01,  1.0864e+00,\n",
      "         9.5805e-01, -1.4053e-01,  1.4219e-01,  5.0391e-01, -5.8840e-01,\n",
      "        -6.1770e-01, -4.2367e-02,  2.9746e-01,  3.9472e-01,  6.5948e-01,\n",
      "         6.4870e-01,  2.8621e-01, -5.4719e-01,  1.5666e+00,  7.9899e-01,\n",
      "        -4.2446e-01,  1.0406e-01,  4.4579e-01,  3.1292e-01,  3.9760e-01,\n",
      "         4.6693e-01,  8.0374e-01, -5.1470e-01, -9.7047e-01,  1.6834e-01,\n",
      "        -3.6884e-01, -4.1780e-01,  2.4287e+00, -5.4882e-01, -8.2594e-01,\n",
      "        -1.1033e-01,  1.7993e-01, -5.7966e-01, -1.0053e+00,  5.4360e-02,\n",
      "         8.3745e-01, -1.8572e-02,  1.1848e+00,  1.1216e+00,  9.6871e-02,\n",
      "         6.3155e-01,  1.8133e-01,  8.8556e-01,  1.8355e-02, -5.7767e-01,\n",
      "        -8.8372e-02, -3.8994e-01, -2.1865e-01, -2.6552e+00,  1.2434e+00,\n",
      "        -5.8577e-02, -6.1316e-01,  8.5756e-01,  4.6876e-01,  1.3117e-01,\n",
      "        -4.7934e-01, -9.2976e-01, -4.2786e-01, -1.2408e+00, -1.0706e+00,\n",
      "         8.2764e-01,  4.4291e-01,  9.3939e-03, -4.5428e-01,  5.5158e-01,\n",
      "         4.7812e-01, -2.3170e-01,  7.4008e-01, -8.4432e-02, -8.8175e-01,\n",
      "        -3.7270e-01,  3.4943e-01,  5.0653e-01,  9.5674e-01, -2.6499e-01,\n",
      "         6.0237e-01,  2.9476e-01,  6.6692e-01, -4.0378e-01, -4.7275e-01,\n",
      "         2.9706e-01, -1.0968e-02, -4.7901e-02, -2.0583e-01, -2.4338e-01,\n",
      "        -4.6847e-02,  5.0317e-01, -5.9015e-01,  7.9091e-02,  5.9922e-01,\n",
      "         3.9032e-01,  7.4259e-01,  1.6661e+00, -3.8645e-01,  6.8543e-02,\n",
      "         4.3226e-01, -4.2901e-01,  7.3505e-01,  4.9009e-01,  2.7700e-01,\n",
      "        -3.1625e-01, -2.6357e-01,  5.4675e-01,  6.0926e-01,  4.2210e-01,\n",
      "         9.3381e-01,  3.0226e-01,  3.2522e-02, -5.8780e-01, -2.1337e-01,\n",
      "         2.1275e-01, -2.7158e-01, -2.2421e-01, -4.9844e-01, -1.3258e+00,\n",
      "        -8.7811e-01,  8.5546e-02,  1.9364e-01, -4.5103e-01,  1.1047e-01,\n",
      "         9.4906e-03, -9.3592e-01, -6.6086e-01, -6.2556e-01,  7.9328e-01,\n",
      "         5.0921e-01,  1.6725e-01,  1.2315e-01,  4.3930e-01, -2.6082e-01,\n",
      "         3.3873e-01, -6.6649e-01, -5.6052e-02,  1.3765e-01, -4.9743e-01,\n",
      "        -4.6564e-01, -1.1718e+00, -7.7288e-01,  4.9258e-01,  1.0411e+00,\n",
      "         3.9725e-01, -1.1614e+00, -3.4327e-01,  2.5483e-01, -8.2274e-01,\n",
      "        -2.2485e-01,  3.2781e-03, -1.7640e-01,  4.3532e-02,  1.5662e+00,\n",
      "         1.7924e-01, -5.5276e-02, -2.1097e-01,  9.5104e-03, -3.9506e-01,\n",
      "         1.0361e-01, -1.2626e+00,  8.9531e-01,  1.0269e+00, -1.1605e+00,\n",
      "         1.1192e+00, -9.7013e-01, -5.5257e-02,  8.7882e-01,  6.6064e-01,\n",
      "        -7.3098e-01,  2.5208e-01, -2.0473e-01, -2.0568e-01, -1.6067e-01,\n",
      "        -2.3325e-01, -9.7818e-01, -4.1126e-01, -1.0732e-01,  5.1858e-01,\n",
      "        -8.4810e-01, -5.9603e-01, -4.0070e-01,  5.6848e-01,  1.7639e-01,\n",
      "        -3.6300e-01,  2.1047e-01,  5.3676e-01,  5.3000e-01,  1.9794e-02,\n",
      "        -4.6304e-01,  4.4807e-01, -3.1348e-01,  7.0238e-01,  7.7698e-01,\n",
      "        -2.5306e-01, -5.6150e-02, -7.1272e-01, -5.2060e-01, -7.4560e-01,\n",
      "         4.0661e-01, -1.6192e-03,  4.5005e-02, -8.9174e-01, -8.6501e-02,\n",
      "         3.1608e-01,  2.6019e-01,  3.5316e-01,  1.8502e-02,  7.2444e-01,\n",
      "        -3.1785e-01, -4.5717e-01, -1.0012e+00,  4.5752e-01, -2.4606e-01,\n",
      "         1.2163e+00,  3.8808e-01, -1.1076e+00,  5.3402e-02, -8.9885e-01,\n",
      "        -7.1840e-01,  1.2101e-01,  6.2478e-01, -7.3701e-01, -2.4266e-01,\n",
      "         7.0464e-01,  5.6331e-01,  2.3871e-01,  1.5987e-01, -4.3172e-01,\n",
      "        -2.0836e-01,  4.0169e-01,  5.3638e-02, -1.6520e-01, -2.2658e-01,\n",
      "        -4.9447e-01, -6.9217e-01,  7.6663e-01,  1.1716e+00,  1.9566e-02,\n",
      "         3.8683e-01, -1.7747e-01, -2.4633e-01, -8.0655e-01,  1.0132e+00,\n",
      "        -2.3439e-01, -3.7830e-01, -7.0372e-01, -1.1921e+00,  2.5819e-01,\n",
      "         8.8446e-02, -5.5196e-01,  5.2032e-01, -2.4670e-01,  2.8241e-01,\n",
      "        -2.1413e-01,  1.3306e-01,  2.3173e-01,  6.3649e-01,  4.1812e-03,\n",
      "        -1.5834e-01, -8.4984e-01,  9.9104e-01, -3.0661e-01, -7.2182e-01,\n",
      "        -1.3686e+00, -2.3150e-01, -1.7500e-01, -4.1266e-01,  7.4226e-01,\n",
      "        -3.1678e-01,  1.9286e-01,  3.3245e-01, -8.8161e-01, -6.4049e-01,\n",
      "        -1.2993e-01,  1.5420e-01, -1.5783e+00, -1.8973e-01, -2.3346e-01,\n",
      "         6.4414e-01, -2.8128e-01,  9.2800e-01, -5.1465e-01, -2.2272e-01,\n",
      "         3.5721e-01,  7.8461e-02, -1.1538e-01,  1.1758e-02, -5.2027e-01,\n",
      "        -1.4293e+00,  2.3791e-01, -1.7786e-01, -9.7017e-01, -3.7604e-02,\n",
      "         2.4901e-01,  2.7699e-01, -2.3343e-01,  6.7893e-01,  3.0562e-01,\n",
      "         1.0287e+00, -5.1488e-03,  7.8724e-01,  9.2038e-01,  1.1320e-01,\n",
      "         2.9870e-01,  3.5950e-01, -8.0323e-01,  5.0228e-01, -4.1497e-01,\n",
      "         1.4397e-02,  1.4573e-01,  5.1800e-01,  3.1016e-01, -8.6760e-01,\n",
      "        -6.5939e-01, -5.2958e-03,  7.3445e-01,  9.4478e-03,  1.0323e-01,\n",
      "         4.7411e-01, -2.3847e-02, -3.6831e-01,  4.7018e-01, -1.1345e+00,\n",
      "        -3.2645e-01, -5.1597e-01,  4.1337e-03, -1.5856e-01, -1.6664e+00,\n",
      "        -1.0176e+00,  8.1060e-02,  8.5260e-02, -9.0039e-01,  3.1661e-01,\n",
      "        -2.5163e-01,  6.1876e-01, -9.1736e-01, -9.8474e-02, -9.7167e-01,\n",
      "         2.6413e-01, -5.0671e-01, -5.5369e-01,  2.3438e-01,  9.5259e-01,\n",
      "        -8.3580e-03, -1.9723e-01, -2.3293e-01,  6.7927e-01,  7.5865e-01,\n",
      "        -9.3720e-02,  2.5393e-03,  2.2441e-01,  7.4625e-02, -5.8992e-01,\n",
      "         4.0699e-01,  3.4996e-01,  3.1125e-01,  2.8247e-01, -3.4963e-01,\n",
      "        -2.7337e-01,  3.0089e-01,  4.0446e-01, -1.1853e+00, -1.2064e+00,\n",
      "         6.5552e-01,  2.1645e-01, -1.6714e+00,  1.0232e-01,  2.1132e-02,\n",
      "        -2.5213e-01, -1.1119e+00,  7.0768e-01, -1.5141e-01,  3.0435e-01,\n",
      "         5.7082e-02,  3.1857e-01, -1.2192e-01,  6.6814e-01, -3.6742e-02,\n",
      "        -4.8797e-01,  1.2045e-01, -1.2279e-01, -2.7191e-01,  4.5842e-01,\n",
      "        -4.1853e-01,  1.4959e-01, -8.9580e-03,  2.4565e-01, -3.4461e-01,\n",
      "        -4.1192e-01,  4.5060e-01, -9.1113e-02,  4.8892e-01, -5.1494e-01,\n",
      "        -5.9131e-01,  1.1115e+00,  3.4217e-02, -4.8535e-01,  1.7315e-01,\n",
      "        -2.4216e-01,  9.9814e-01,  5.4380e-01, -1.5811e-03,  4.8767e-01,\n",
      "        -4.8005e-01, -7.4016e-01,  2.6312e-01,  2.4377e-01,  3.4794e-01,\n",
      "        -1.2807e-01, -6.9131e-01,  1.0789e+00,  3.6272e-01,  4.4604e-01,\n",
      "        -3.3334e-01, -6.0666e-01,  6.0547e-02,  2.5235e-01, -3.3373e-01,\n",
      "        -7.2425e-01,  1.9860e-02,  5.7691e-01, -2.3622e-01, -2.7402e-01,\n",
      "         1.2380e+00,  2.7653e-01, -3.1691e-01, -3.7992e-01, -6.5465e-01,\n",
      "         2.4356e-01,  2.4057e-01, -9.3775e-01,  3.3488e-01, -6.2111e-01,\n",
      "        -5.2515e-01, -5.4167e-01, -2.5588e-01, -8.5824e-01,  1.0152e+00,\n",
      "        -6.3203e-01, -1.2432e-01, -4.7801e-01,  2.7275e-01,  5.6323e-01,\n",
      "        -5.5672e-01,  5.2269e-01,  1.9409e-01, -9.8205e-01,  4.7863e-01,\n",
      "         2.5835e-02,  9.7691e-01,  2.1451e-01,  4.0859e-01, -3.5870e-01,\n",
      "         3.2623e-02,  1.3111e-01,  3.7986e-01, -6.5166e-02,  9.7554e-01,\n",
      "        -1.1027e-01, -1.0769e+00,  3.3600e-01,  5.5230e-01,  6.4527e-01,\n",
      "        -4.4907e-01,  1.1377e+00, -3.9828e-01, -7.5065e-01,  4.5029e-01,\n",
      "         2.8535e-01, -2.2207e-01,  6.3101e-02, -8.6954e-02,  6.0205e-01,\n",
      "        -3.4972e-01, -4.2345e-02, -9.0807e-01, -4.0555e-01,  1.6154e-02,\n",
      "         4.2367e-01,  8.8251e-02,  4.9467e-01, -1.1219e+00, -2.9992e-01,\n",
      "         4.9874e-01,  1.6081e+00, -3.0927e-01,  1.9078e-02,  3.3181e-01,\n",
      "         3.2845e-02, -2.6813e-01, -2.6807e-01,  1.1537e+00,  1.5491e-01,\n",
      "         4.2065e-01,  4.3847e-01,  7.1526e-01, -4.2298e-02, -6.4447e-01,\n",
      "        -4.5419e-01, -4.6788e-01, -5.3283e-01, -9.5000e-01, -2.7426e-01,\n",
      "        -2.5471e-01, -5.3061e-01,  1.1504e-01, -2.8412e-01, -4.6136e-01,\n",
      "        -6.2779e-01, -5.1189e-01, -1.1935e-01])\n",
      "\n",
      "Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sentence\n",
    "sentence = \"He went to the bank to deposit money.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Get outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get hidden states (embeddings)\n",
    "embeddings = outputs.last_hidden_state.squeeze(0)  # shape: (seq_len, hidden_size)\n",
    "# Print tokens and their embeddings\n",
    "for token, embedding in zip(tokens, embeddings):\n",
    "    print(f\"Token: {token}\\n Embedding(First 10 Numbers):\\n{embedding[:10]}\\nShape: {embedding.shape}\\n\")\n",
    "\n",
    "\n",
    "# # Find index of \"bank\"\n",
    "# try:\n",
    "#     idx = tokens.index(\"bank\")\n",
    "#     bank_embedding = embeddings[idx]\n",
    "#     print(f\"Embedding for 'bank':\\n{bank_embedding}\\n\\nShape: {bank_embedding.shape}\")\n",
    "# except ValueError:\n",
    "#     print(\"'bank' not found in tokenized input:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eefff",
   "metadata": {},
   "source": [
    "#### Use BERT to Create Context-Aware Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21998295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' (fruit) and 'orange': 0.5839\n",
      "Similarity between 'apple' (company) and 'Microsoft': 0.8549\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract contextual embedding for a word (handles subwords)\n",
    "def get_token_embedding(sentence, target_word):\n",
    "    # Tokenize the sentence and get embeddings\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get tokens and embeddings\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # Tokenize the target word the same way BERT does\n",
    "    target_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "    # Search for the position of the target word (handling subwords)\n",
    "    matches = []\n",
    "    for i in range(len(tokens) - len(target_tokens) + 1):\n",
    "        if tokens[i:i + len(target_tokens)] == target_tokens:\n",
    "            matches = list(range(i, i + len(target_tokens)))\n",
    "            break\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(f\"'{target_word}' not found in tokens: {tokens}\")\n",
    "\n",
    "    # Average the embeddings over all subword tokens\n",
    "    return embeddings[matches].mean(dim=0)\n",
    "\n",
    "# Contextual sentences\n",
    "sentence_fruit = \"He ate a fresh apple and enjoyed the fruit.\"\n",
    "sentence_company = \"Apple released a new product in the computer market.\"\n",
    "sentence_orange = \"An orange is a juicy fruit.\"\n",
    "sentence_microsoft = \"Microsoft computer was running the latest software.\"\n",
    "\n",
    "# Get embeddings\n",
    "apple_fruit = get_token_embedding(sentence_fruit, \"apple\")\n",
    "apple_company = get_token_embedding(sentence_company, \"apple\")\n",
    "orange = get_token_embedding(sentence_orange, \"orange\")\n",
    "microsoft = get_token_embedding(sentence_microsoft, \"Microsoft\")\n",
    "\n",
    "# Cosine similarity comparisons\n",
    "sim_fruit = F.cosine_similarity(apple_fruit, orange, dim=0)\n",
    "sim_company = F.cosine_similarity(apple_company, microsoft, dim=0)\n",
    "\n",
    "# Results\n",
    "print(f\"Similarity between 'apple' (fruit) and 'orange': {sim_fruit.item():.4f}\")\n",
    "print(f\"Similarity between 'apple' (company) and 'Microsoft': {sim_company.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e4ba7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05ecb6",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008fd48",
   "metadata": {},
   "source": [
    "Basic Pipeline Usage\n",
    "1. Text Classification (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze single text\n",
    "result = classifier(\"I love using Hugging Face!\")\n",
    "print(result)\n",
    "# Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Analyze multiple texts\n",
    "texts = [\n",
    "    \"I hate this product\",\n",
    "    \"This is amazing!\",\n",
    "    \"It's okay, nothing special\"\n",
    "]\n",
    "results = classifier(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce5a1",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"My name is John and I live in New York. I work at Google.\"\n",
    "entities = ner(text)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}\")\n",
    "    print(f\"Label: {entity['entity_group']}\")\n",
    "    print(f\"Score: {entity['score']:.4f}\")\n",
    "    print(f\"Start: {entity['start']}, End: {entity['end']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0c3cc",
   "metadata": {},
   "source": [
    "3. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning. \n",
    "They are especially known for their work in natural language processing. The company was founded in 2016 \n",
    "and is headquartered in New York.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"What is Hugging Face known for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890032c0",
   "metadata": {},
   "source": [
    "4. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text with custom parameters\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where robots exist,\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    for i, gen in enumerate(generated):\n",
    "        print(f\"Generation {i+1}: {gen['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb32898",
   "metadata": {},
   "source": [
    "5. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ff021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "article = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve \n",
    "from experience without being explicitly programmed. It focuses on the development of computer programs \n",
    "that can access data and use it to learn for themselves. The process of learning begins with observations \n",
    "or data, such as examples, direct experience, or instruction, in order to look for patterns in data and \n",
    "make better decisions in the future based on the examples that we provide. The primary aim is to allow \n",
    "the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Original length:\", len(article.split()))\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n",
    "print(\"Summary length:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e35a94",
   "metadata": {},
   "source": [
    "6. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I would like to order a coffee.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    translated = translator(text)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"French: {translated[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8e4b",
   "metadata": {},
   "source": [
    "#### Use specific model e.g. BERT to Create Questions Answering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a0246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\me\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a method of pre-training language representations\n",
      "Confidence: 0.6874\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering \n",
    "from transformers import pipeline \n",
    "import torch \n",
    "\n",
    "# Using pipeline (High-level API) \n",
    "qa_pipeline = pipeline( \"question-answering\",\n",
    "model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\" ) \n",
    "\n",
    "# Example usage \n",
    "context = \"\"\" BERT is a method of pre-training language representations, \n",
    "meaning that it trains a general-purpose language understanding \n",
    "model on a large text corpus (like Wikipedia), \n",
    "and then uses that model for downstream NLP tasks like question answering. \"\"\" \n",
    "\n",
    "question = \"What is BERT?\" \n",
    "result = qa_pipeline(question=question, context=context) \n",
    "print(f\"Answer: {result['answer']}\") \n",
    "print(f\"Confidence: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416b4ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fad4f2",
   "metadata": {},
   "source": [
    "# **Fine Tuning Large Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc745b71",
   "metadata": {},
   "source": [
    "#### **Tutorial: Fine-tuning a Language Model and Deploying with Hugging Face Spaces - Sentiment Analysis IMDB Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515fcac",
   "metadata": {},
   "source": [
    "#### **Step 1: Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd582702",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets huggingface_hub gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadf437",
   "metadata": {},
   "source": [
    "#### **Step 2: Load and Prepare the Dataset**\n",
    "We will use a small portion of the IMDb dataset for binary sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small subset for quicker training\n",
    "# dataset = load_dataset(\"imdb\", split=\"train\", download_mode=\"force_redownload\")\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a042c32",
   "metadata": {},
   "source": [
    "#### **Step 3: Load the Tokenizer and Model**\n",
    "\n",
    "We use `distilbert-base-uncased`, a lightweight version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74f318",
   "metadata": {},
   "source": [
    "#### **Step 4: Tokenize the Dataset**\n",
    "Tokenization prepares the text for input to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ec5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e9377",
   "metadata": {},
   "source": [
    "#### **Step 5: Define Training Arguments and Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbf260",
   "metadata": {},
   "source": [
    "#### **Step 6: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da83116",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f24e00",
   "metadata": {},
   "source": [
    "#### **Step 7: Log in to Hugging Face Hub**\n",
    "\n",
    "Do this only when you're ready to push your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79683e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f9bb7",
   "metadata": {},
   "source": [
    "After running this cell, youâ€™ll be prompted to enter your Hugging Face access token. You can create one here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82a9f7",
   "metadata": {},
   "source": [
    "#### **Step 8: Push Model and Tokenizer to Hugging Face Hub**\n",
    "\n",
    "Replace `\"your-username/model-name\"` with your actual username and desired model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"your-username/distilbert-sentiment-imdb-small\"\n",
    "\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53777f",
   "metadata": {},
   "source": [
    "This makes your model available for download and use in a Hugging Face Space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04edc99",
   "metadata": {},
   "source": [
    "#### **Step 9: (Optional) Test with Gradio Locally in Colab**\n",
    "\n",
    "This is useful for debugging before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8dbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    result = classifier(text)[0]\n",
    "    return f\"Label: {result['label']}, Confidence: {round(result['score'], 3)}\"\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Sentiment Analysis\",\n",
    "    description=\"Enter a movie review to classify as POSITIVE or NEGATIVE.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a64ab4",
   "metadata": {},
   "source": [
    "#### **Step 10: Create a Hugging Face Space for Deployment**\n",
    "\n",
    "1. Go to [https://huggingface.co/spaces](https://huggingface.co/spaces)\n",
    "2. Click \"Create New Space\"\n",
    "3. Choose:\n",
    "\n",
    "   * **SDK**: Gradio\n",
    "   * **Visibility**: Public or Private\n",
    "   * Name: e.g. `sentiment-analyzer-student`\n",
    "\n",
    "Add these two files to your Space:\n",
    "\n",
    "1. `app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35100cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"your-username/distilbert-sentiment-imdb-small\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    result = classifier(text)[0]\n",
    "    return f\"Label: {result['label']}, Confidence: {round(result['score'], 3)}\"\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Sentiment Analysis\",\n",
    "    description=\"Enter a movie review to classify as POSITIVE or NEGATIVE.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acd979",
   "metadata": {},
   "source": [
    "2. `requirements.txt`\n",
    "```\n",
    "transformers\n",
    "torch\n",
    "gradio\n",
    "```\n",
    "\n",
    "After uploading both files, Hugging Face will automatically build and deploy your Space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932daf32",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "This complete workflow demonstrates how to:\n",
    "\n",
    "* Fine-tune a transformer model on a small dataset\n",
    "* Save and share the model using Hugging Face Hub\n",
    "* Deploy the model as a web app with Hugging Face Spaces and Gradio\n",
    "\n",
    "This structure is optimized for educational use, minimal setup, and reproducibility. If you would like a Colab version or a GitHub template, I can generate those for you as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a308a29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66f008",
   "metadata": {},
   "source": [
    "### Full Fine Tuning Code - IMDB Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da74fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(\"imdb\", split='train[:2000]')\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Step 2: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Step 3: Tokenize\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, batched=True)\n",
    "tokenized_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Step 4: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Step 5: Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Step 6: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace17c1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b67b3",
   "metadata": {},
   "source": [
    "### **Full Fine Tuning Code - Sentiment Analysis Amazon Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations (uncomment if not already installed)\n",
    "# !pip install transformers datasets scikit-learn\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file (replace with your actual path)\n",
    "df = pd.read_csv(\"amazon_reviews.csv\")  # Columns: 'title', 'content', 'label'\n",
    "\n",
    "# Combine title and content for input\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"content\"]\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Split into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load BERT model for binary classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer for training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "# Save the model\n",
    "trainer.save_model(\"fine_tuned_bert_amazon_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714067a",
   "metadata": {},
   "source": [
    "### **More Fine Tuning Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713579e",
   "metadata": {},
   "source": [
    "### **1. Text Classification: News Topic Classification**\n",
    "\n",
    "#### Task: Classify news articles into topics (e.g., business, sports, politics)\n",
    "\n",
    "* **Dataset**: AG News (4-class classification)\n",
    "* **Model**: `distilbert-base-uncased`\n",
    "* **Why it's good**: Multiclass instead of binary; introduces students to topic classification.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_data = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "Then follow the same login + push + deploy steps.\n",
    "\n",
    "### **2. Text Generation: Simple Story Completion (using GPT-2)**\n",
    "\n",
    "#### Task: Given a prompt, generate the next few sentences of a story.\n",
    "\n",
    "* **Model**: `gpt2`\n",
    "* **Dataset**: A small set of fairy tales or a pre-tokenized open dataset like `wikitext`\n",
    "\n",
    "> GPT-based fine-tuning takes longer and needs GPU memory, so keep the dataset very small.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load small dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no padding token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "You can then build a Gradio app with a text input (prompt) and text output (generated continuation).\n",
    "\n",
    "### **3. Named Entity Recognition (NER)**\n",
    "\n",
    "#### Task: Identify entities like person names, locations, etc.\n",
    "\n",
    "* **Dataset**: `conll2003`\n",
    "* **Model**: `bert-base-cased`\n",
    "\n",
    "NER gives students exposure to **token-level** classification.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=\"max_length\")\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data.set_format(\"torch\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=9)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"].select(range(1000)),\n",
    "    eval_dataset=tokenized_data[\"validation\"].select(range(200)),\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
