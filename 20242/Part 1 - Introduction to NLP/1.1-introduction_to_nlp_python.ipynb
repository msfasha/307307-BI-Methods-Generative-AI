{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://raw.githubusercontent.com/msfasha/307307-BI-Methods/main/20242-NLP-LLM/images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to Natural Language Processing and Classical Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "    <a href=\"https://github.com/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/Part%201%20-%20Introduction%20to%20NLP/1.1-introduction_to_nlp_python.ipynb\" target=\"_blank\">    \n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 25px; margin-right: 20px;\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1:- Introduction to Basic NLP Opertations**\n",
    "- Tokenization\n",
    "- Normaliztion\n",
    "- Stopwords Removal\n",
    "- Stemming and Lemmatization\n",
    "- Representing Text:\n",
    "    -   Bag of Words (BoW)\n",
    "    -   Term-Frequency Invesrse Term Frequency (TD/IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NLTK from the Internet and install it on our PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/me/myenv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/me/myenv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/me/myenv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/me/myenv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/me/myenv/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download other neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/me/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/me/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/me/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords') # stopwords are common words that are often removed from text as they are not useful for analysis.\n",
    "nltk.download('wordnet') # for nltk.stem using WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import NLTK so that we can use it in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize words using NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# punkt_tab is a pretrained tokenization model used for splitting text into sentences and words.\n",
    "\n",
    "sentence = \"Large language models are revolutionizing business applications.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Converting text to a standard form to reduce variability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert all letters to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'language', 'models', 'are', 'revolutionizing', 'business', 'applications', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# [^\\w\\s] means any character that is not a word character or whitespace, ^ inside square brackets negates the expression.\n",
    "# \\w is a word character (alphanumeric character plus underscore)\n",
    "normalized_tokens = [re.sub(r'[^\\w\\s]', '', token.lower()) for token in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods/blob/main/20242-NLP-LLM/lecture%20notes/Part%201%20-%20Introduction%20to%20NLP/introduction_to_regular_expressions.ipynb\" \n",
    " target=\"_blank\">                                                          \n",
    "Click here to open regular expressions tutorial</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal:\n",
    "Eliminating common words that add little meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the stopwords dataset and import it in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These words are removed from the text:  {'d', 'why', \"you've\", 'have', 'hadn', 'again', 'by', 'her', 'is', \"she'll\", 'each', 'very', 'while', 'are', \"he'll\", 'should', 'this', 'shan', 'o', 'against', 'over', 'will', 'themselves', 'who', \"they'd\", \"we've\", 'out', 'between', \"mustn't\", 're', 'she', 'yourselves', 'myself', \"we're\", 'them', \"he'd\", 'that', 'haven', 'here', 'couldn', 'needn', 'before', 'under', \"hadn't\", 'no', 'we', 'of', 'weren', 'until', \"you'd\", 'ain', 'hasn', 'after', 'don', 't', \"he's\", \"weren't\", 'theirs', \"they're\", 'or', 'because', 'the', 'below', 'nor', 'was', 'herself', 've', 'not', 'isn', 'm', 'any', \"it's\", 'do', 'had', 'at', 'if', 'all', 'more', 'our', 'y', 'for', \"you'll\", \"won't\", \"should've\", 'in', \"haven't\", 'now', 'once', \"she's\", 'doesn', 'll', 'during', 'ma', 'when', \"aren't\", \"i'd\", 'can', 'ours', 'me', 'its', 'some', 'himself', 'wasn', 'to', 'than', \"couldn't\", 'aren', \"shouldn't\", 'these', \"you're\", \"needn't\", 'your', \"it'll\", 'through', 'which', 'above', 'hers', 'most', 'being', 'wouldn', \"they've\", 'him', 'only', 'whom', \"i'll\", 'there', 'does', 'own', 'won', 'having', 'it', 'then', 'mustn', 'been', 'other', 'too', \"they'll\", \"didn't\", \"we'll\", 'but', 'did', 'has', \"i'm\", 'further', 'a', 'on', 'their', 'an', 'were', 'into', 'you', 'be', 'so', \"shan't\", 'shouldn', 'his', 'he', 'where', 'didn', 'down', 'how', 'am', 'and', 'they', 'such', \"mightn't\", 'what', 'as', 'few', \"it'd\", \"isn't\", \"i've\", 'itself', 'with', 'doing', \"don't\", 'those', 'yours', 'ourselves', \"we'd\", 'same', 'from', \"wasn't\", \"doesn't\", \"that'll\", \"hasn't\", 'i', 'up', 'off', \"wouldn't\", 'yourself', 'just', 'about', 'my', \"she'd\", 'mightn', 'both', 's'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"These words are removed from the text: \", stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can remove stopwords from our sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'language', 'models', 'revolutionizing', 'business', 'applications']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [token for token in normalized_tokens if token and token not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming vs. Lemmatization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larg', 'languag', 'model', 'revolution', 'busi', 'applic']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'language', 'model', 'revolutionizing', 'business', 'application']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Text - Bag of Words and TF-IDF\n",
    "#### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to represent text as numerical vectors by counting word occurrences and representing each sentence as a vector or word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ai",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "applications",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "benefit",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "business",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "data",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "from",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "language",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "large",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "models",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "revolutionize",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "77d6d631-cf56-4653-b0e0-80b70920195a",
       "rows": [
        [
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0"
        ],
        [
         "1",
         "1",
         "1",
         "1",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0",
         "1"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>applications</th>\n",
       "      <th>benefit</th>\n",
       "      <th>business</th>\n",
       "      <th>data</th>\n",
       "      <th>from</th>\n",
       "      <th>language</th>\n",
       "      <th>large</th>\n",
       "      <th>learn</th>\n",
       "      <th>models</th>\n",
       "      <th>revolutionize</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  applications  benefit  business  data  from  language  large  learn  \\\n",
       "0   0             0        0         1     0     0         1      1      0   \n",
       "1   1             1        1         1     0     1         0      0      0   \n",
       "2   0             0        0         0     1     1         1      0      1   \n",
       "\n",
       "   models  revolutionize  text  \n",
       "0       1              1     0  \n",
       "1       0              0     0  \n",
       "2       1              0     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Large language models revolutionize business.\",\n",
    "    \"Business applications benefit from AI.\",\n",
    "    \"Language models learn from text data.\"\n",
    "]\n",
    "\n",
    "#  CountVectorizer is used to convert a collection of text documents to a matrix of token counts.\n",
    "#  The output is a sparse matrix where each row represents a document and each column represents a word in the corpus.\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names and the array representation of the matrix\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ai",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "applications",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "benefit",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "business",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "data",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "from",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "language",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "large",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "learn",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "models",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "revolutionize",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "30c26c93-7715-4f19-8b39-9ca3e9b2e096",
       "rows": [
        [
         "0",
         "0.0",
         "0.0",
         "0.0",
         "0.39351120409397233",
         "0.0",
         "0.0",
         "0.39351120409397233",
         "0.5174199439321682",
         "0.0",
         "0.39351120409397233",
         "0.5174199439321682",
         "0.0"
        ],
        [
         "1",
         "0.49047908420610337",
         "0.49047908420610337",
         "0.49047908420610337",
         "0.3730219858594306",
         "0.0",
         "0.3730219858594306",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.45954803293870056",
         "0.3494981241087058",
         "0.3494981241087058",
         "0.0",
         "0.45954803293870056",
         "0.3494981241087058",
         "0.0",
         "0.45954803293870056"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>applications</th>\n",
       "      <th>benefit</th>\n",
       "      <th>business</th>\n",
       "      <th>data</th>\n",
       "      <th>from</th>\n",
       "      <th>language</th>\n",
       "      <th>large</th>\n",
       "      <th>learn</th>\n",
       "      <th>models</th>\n",
       "      <th>revolutionize</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>0.51742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459548</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.459548</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.459548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  applications   benefit  business      data      from  language  \\\n",
       "0  0.000000      0.000000  0.000000  0.393511  0.000000  0.000000  0.393511   \n",
       "1  0.490479      0.490479  0.490479  0.373022  0.000000  0.373022  0.000000   \n",
       "2  0.000000      0.000000  0.000000  0.000000  0.459548  0.349498  0.349498   \n",
       "\n",
       "     large     learn    models  revolutionize      text  \n",
       "0  0.51742  0.000000  0.393511        0.51742  0.000000  \n",
       "1  0.00000  0.000000  0.000000        0.00000  0.000000  \n",
       "2  0.00000  0.459548  0.349498        0.00000  0.459548  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Large language models revolutionize business.\",\n",
    "    \"Business applications benefit from AI.\",\n",
    "    \"Language models learn from text data.\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the corpus into a document-term matrix\n",
    "# Each row represents a document in the corpus\n",
    "# Each column represents a unique word in the corpus\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Practical Example**\n",
    "### **Sentiment Analysis of Movie Reviews**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application will go through the entire text preprocessing pipeline and show how it contributes to a real-world NLP task.\n",
    "Setup\n",
    "\n",
    "Dataset: Use a small dataset of movie reviews (positive and negative) - you could use a subset of IMDB reviews or create 10-15 simple examples.\n",
    "Visual Flow: Create a slide that shows the entire pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing Pipeline\n",
    "**Raw Text → Tokenization → Normalization → Stop Words Removal → Stemming/Lemmatization → BoW → Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do the following:\n",
    "- Import the needed libraries\n",
    "- Download any additional modules that are arequired\n",
    "- Prepare the dataset i.e Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                                                text  sentiment\n",
      "0  This movie was absolutely fantastic! Great act...          1\n",
      "1  I loved this film. The characters were so well...          1\n",
      "2  Amazing cinematography and directing. One of t...          1\n",
      "3  The acting was good but the story was too pred...          0\n",
      "4     Terrible movie. I wasted two hours of my life.          0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    {\"text\": \"This movie was absolutely fantastic! Great acting and storyline.\", \"sentiment\": 1},\n",
    "    {\"text\": \"I loved this film. The characters were so well developed.\", \"sentiment\": 1},\n",
    "    {\"text\": \"Amazing cinematography and directing. One of the best films I've seen.\", \"sentiment\": 1},\n",
    "    {\"text\": \"The acting was good but the story was too predictable.\", \"sentiment\": 0},\n",
    "    {\"text\": \"Terrible movie. I wasted two hours of my life.\", \"sentiment\": 0},\n",
    "    {\"text\": \"The special effects were amazing but everything else was boring.\", \"sentiment\": 0},\n",
    "    {\"text\": \"I enjoyed the action sequences but the dialogue was poorly written.\", \"sentiment\": 0},\n",
    "    {\"text\": \"Brilliant performance by the lead actor! Highly recommended.\", \"sentiment\": 1},\n",
    "    {\"text\": \"So disappointing. The trailer was better than the actual movie.\", \"sentiment\": 0},\n",
    "    {\"text\": \"A masterpiece of modern cinema. I was captivated throughout.\", \"sentiment\": 1}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(reviews)\n",
    "print(\"Original Data:\")\n",
    "print(df.head())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text segment below, we will define the required text processing functions:\n",
    "1. tokenize text function\n",
    "2. normalize_tokens function\n",
    "3. remove_stopwords function\n",
    "4. stem_tokens function\n",
    "5. lemmatize_tokens function\n",
    "6. preprocess_text function, the pipeline that calls all the previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING PIPELINE DEMONSTRATION:\n",
      "Original text: 'This movie was absolutely fantastic! Great acting and storyline.'\n",
      "Processed tokens: ['movi', 'absolut', 'fantast', 'great', 'act', 'storylin']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(f\"Sentence after tokenization: {tokens}\") # uncomment if need to see results\n",
    "    return tokens\n",
    "\n",
    "# Step 2: Normalization\n",
    "def normalize_tokens(tokens):\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    normalized = [re.sub(r'[^\\w\\s]', '', token.lower()) for token in tokens]\n",
    "    normalized = [token for token in normalized if token]  # Remove empty strings\n",
    "    # print(f\"Tokens after normalization: {normalized}\") # uncomment if need to see results\n",
    "    return normalized\n",
    "\n",
    "# Step 3: Remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [token for token in tokens if token not in stop_words]\n",
    "    # print(f\"Tokens after stopword removal: {filtered}\") # uncomment if need to see results\n",
    "    return filtered\n",
    "\n",
    "# Step 4a: Stemming\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    # print(f\"Tokesn after stemming: {stemmed}\") # uncomment if need to see results\n",
    "    return stemmed\n",
    "\n",
    "# Step 4b: Lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # print(f\"Tokens after lemmatization: {lemmatized}\") # uncomment if need to see results\n",
    "    return lemmatized\n",
    "\n",
    "# Complete preprocessing pipeline\n",
    "def preprocess_text(text, use_stemming=True):\n",
    "    tokens = tokenize_text(text)\n",
    "    normalized_tokens = normalize_tokens(tokens)\n",
    "    no_stopwords = remove_stopwords(normalized_tokens)\n",
    "    \n",
    "    if use_stemming:\n",
    "        return stem_tokens(no_stopwords)\n",
    "    else:\n",
    "        return lemmatize_tokens(no_stopwords)\n",
    "\n",
    "# Demonstrate the preprocessing pipeline on one example\n",
    "print(\"PREPROCESSING PIPELINE DEMONSTRATION:\")\n",
    "sample_text = df['text'][0]\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "processed_tokens = preprocess_text(sample_text, use_stemming=True)\n",
    "print(f\"Processed tokens: {processed_tokens}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the text processing pipeline on all our sentences (entire text corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "processed_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9bb8f03f-6b71-41ca-91ea-0566244dc0b5",
       "rows": [
        [
         "0",
         "This movie was absolutely fantastic! Great acting and storyline.",
         "1",
         "movi absolut fantast great act storylin"
        ],
        [
         "1",
         "I loved this film. The characters were so well developed.",
         "1",
         "love film charact well develop"
        ],
        [
         "2",
         "Amazing cinematography and directing. One of the best films I've seen.",
         "1",
         "amaz cinematographi direct one best film seen"
        ],
        [
         "3",
         "The acting was good but the story was too predictable.",
         "0",
         "act good stori predict"
        ],
        [
         "4",
         "Terrible movie. I wasted two hours of my life.",
         "0",
         "terribl movi wast two hour life"
        ],
        [
         "5",
         "The special effects were amazing but everything else was boring.",
         "0",
         "special effect amaz everyth els bore"
        ],
        [
         "6",
         "I enjoyed the action sequences but the dialogue was poorly written.",
         "0",
         "enjoy action sequenc dialogu poorli written"
        ],
        [
         "7",
         "Brilliant performance by the lead actor! Highly recommended.",
         "1",
         "brilliant perform lead actor highli recommend"
        ],
        [
         "8",
         "So disappointing. The trailer was better than the actual movie.",
         "0",
         "disappoint trailer better actual movi"
        ],
        [
         "9",
         "A masterpiece of modern cinema. I was captivated throughout.",
         "1",
         "masterpiec modern cinema captiv throughout"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was absolutely fantastic! Great act...</td>\n",
       "      <td>1</td>\n",
       "      <td>movi absolut fantast great act storylin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I loved this film. The characters were so well...</td>\n",
       "      <td>1</td>\n",
       "      <td>love film charact well develop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing cinematography and directing. One of t...</td>\n",
       "      <td>1</td>\n",
       "      <td>amaz cinematographi direct one best film seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The acting was good but the story was too pred...</td>\n",
       "      <td>0</td>\n",
       "      <td>act good stori predict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible movie. I wasted two hours of my life.</td>\n",
       "      <td>0</td>\n",
       "      <td>terribl movi wast two hour life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The special effects were amazing but everythin...</td>\n",
       "      <td>0</td>\n",
       "      <td>special effect amaz everyth els bore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I enjoyed the action sequences but the dialogu...</td>\n",
       "      <td>0</td>\n",
       "      <td>enjoy action sequenc dialogu poorli written</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brilliant performance by the lead actor! Highl...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant perform lead actor highli recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So disappointing. The trailer was better than ...</td>\n",
       "      <td>0</td>\n",
       "      <td>disappoint trailer better actual movi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A masterpiece of modern cinema. I was captivat...</td>\n",
       "      <td>1</td>\n",
       "      <td>masterpiec modern cinema captiv throughout</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  This movie was absolutely fantastic! Great act...          1   \n",
       "1  I loved this film. The characters were so well...          1   \n",
       "2  Amazing cinematography and directing. One of t...          1   \n",
       "3  The acting was good but the story was too pred...          0   \n",
       "4     Terrible movie. I wasted two hours of my life.          0   \n",
       "5  The special effects were amazing but everythin...          0   \n",
       "6  I enjoyed the action sequences but the dialogu...          0   \n",
       "7  Brilliant performance by the lead actor! Highl...          1   \n",
       "8  So disappointing. The trailer was better than ...          0   \n",
       "9  A masterpiece of modern cinema. I was captivat...          1   \n",
       "\n",
       "                                  processed_text  \n",
       "0        movi absolut fantast great act storylin  \n",
       "1                 love film charact well develop  \n",
       "2  amaz cinematographi direct one best film seen  \n",
       "3                         act good stori predict  \n",
       "4                terribl movi wast two hour life  \n",
       "5           special effect amaz everyth els bore  \n",
       "6    enjoy action sequenc dialogu poorli written  \n",
       "7  brilliant perform lead actor highli recommend  \n",
       "8          disappoint trailer better actual movi  \n",
       "9     masterpiec modern cinema captiv throughout  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: ' '.join(preprocess_text(x, use_stemming=True)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text into numerical representation using the Bag of Words Method, we will the use the simple count approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Matrix (First 3 Documents):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "absolut",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "act",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "action",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "actor",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "actual",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "amaz",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "best",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "better",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bore",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "brilliant",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "captiv",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "charact",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cinema",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cinematographi",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "develop",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dialogu",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "direct",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disappoint",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "effect",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "els",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "enjoy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "everyth",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fantast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "film",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "good",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "great",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "highli",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lead",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "life",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "love",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "masterpiec",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "modern",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "movi",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "one",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "perform",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "poorli",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "predict",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "recommend",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "seen",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sequenc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "special",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "stori",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "storylin",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "terribl",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "throughout",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trailer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "two",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "well",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "written",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c68ba1b2-a753-467d-8c6f-3f7922c8cba8",
       "rows": [
        [
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0"
        ],
        [
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0"
        ],
        [
         "5",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "6",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1"
        ],
        [
         "7",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "8",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "9",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 51,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolut</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>amaz</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>bore</th>\n",
       "      <th>brilliant</th>\n",
       "      <th>...</th>\n",
       "      <th>special</th>\n",
       "      <th>stori</th>\n",
       "      <th>storylin</th>\n",
       "      <th>terribl</th>\n",
       "      <th>throughout</th>\n",
       "      <th>trailer</th>\n",
       "      <th>two</th>\n",
       "      <th>wast</th>\n",
       "      <th>well</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolut  act  action  actor  actual  amaz  best  better  bore  brilliant  \\\n",
       "0        1    1       0      0       0     0     0       0     0          0   \n",
       "1        0    0       0      0       0     0     0       0     0          0   \n",
       "2        0    0       0      0       0     1     1       0     0          0   \n",
       "3        0    1       0      0       0     0     0       0     0          0   \n",
       "4        0    0       0      0       0     0     0       0     0          0   \n",
       "5        0    0       0      0       0     1     0       0     1          0   \n",
       "6        0    0       1      0       0     0     0       0     0          0   \n",
       "7        0    0       0      1       0     0     0       0     0          1   \n",
       "8        0    0       0      0       1     0     0       1     0          0   \n",
       "9        0    0       0      0       0     0     0       0     0          0   \n",
       "\n",
       "   ...  special  stori  storylin  terribl  throughout  trailer  two  wast  \\\n",
       "0  ...        0      0         1        0           0        0    0     0   \n",
       "1  ...        0      0         0        0           0        0    0     0   \n",
       "2  ...        0      0         0        0           0        0    0     0   \n",
       "3  ...        0      1         0        0           0        0    0     0   \n",
       "4  ...        0      0         0        1           0        0    1     1   \n",
       "5  ...        1      0         0        0           0        0    0     0   \n",
       "6  ...        0      0         0        0           0        0    0     0   \n",
       "7  ...        0      0         0        0           0        0    0     0   \n",
       "8  ...        0      0         0        0           0        1    0     0   \n",
       "9  ...        0      0         0        0           1        0    0     0   \n",
       "\n",
       "   well  written  \n",
       "0     0        0  \n",
       "1     1        0  \n",
       "2     0        0  \n",
       "3     0        0  \n",
       "4     0        0  \n",
       "5     0        0  \n",
       "6     0        1  \n",
       "7     0        0  \n",
       "8     0        0  \n",
       "9     0        0  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "print(\"BOW Matrix (First 3 Documents):\")\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the classifier and train it on the generated BoW representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained classifier to make predictions using the test data (sentences that was not seen by the classifier during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classification results (Model Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.75      0.75      0.67         3\n",
      "weighted avg       0.83      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:- Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that builds the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(text, n=2):\n",
    "    \"\"\"Build an n-gram language model from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngrams_dict = defaultdict(list)\n",
    "    \n",
    "    # Create dictionary of n-grams and possible next words\n",
    "    for i in range(len(tokens) - n):\n",
    "        current_ngram = tuple(tokens[i:i+n])\n",
    "        next_word = tokens[i+n]\n",
    "        ngrams_dict[current_ngram].append(next_word)\n",
    "    \n",
    "    return ngrams_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function that is used to generate new text based on an input seed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, length):\n",
    "    \"\"\"Generate text using the n-gram model.\"\"\"\n",
    "    current = seed\n",
    "    result = list(seed)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Randomly select a possible next word\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current n-gram\n",
    "            current = current[1:] + (next_word,)\n",
    "        else:\n",
    "            # If current n-gram is not in model, break\n",
    "            break\n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sample corups to build our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Dictionary/Model:\n",
      "('large', 'language') → ['models']\n",
      "('language', 'models') → ['are', 'for', 'learn']\n",
      "('models', 'are') → ['transforming']\n",
      "('are', 'transforming') → ['how']\n",
      "('transforming', 'how') → ['businesses']\n",
      "('how', 'businesses') → ['operate']\n",
      "('businesses', 'operate') → ['.']\n",
      "('operate', '.') → ['these']\n",
      "('.', 'these') → ['models']\n",
      "('these', 'models') → ['can']\n",
      "('models', 'can') → ['understand']\n",
      "('can', 'understand') → ['language']\n",
      "('understand', 'language') → [',']\n",
      "('language', ',') → ['generate']\n",
      "(',', 'generate') → ['text']\n",
      "('generate', 'text') → [',']\n",
      "('text', ',') → ['and']\n",
      "(',', 'and') → ['perform', 'data']\n",
      "('and', 'perform') → ['various']\n",
      "('perform', 'various') → ['tasks']\n",
      "('various', 'tasks') → ['.']\n",
      "('tasks', '.') → ['businesses']\n",
      "('.', 'businesses') → ['use']\n",
      "('businesses', 'use') → ['language']\n",
      "('use', 'language') → ['models']\n",
      "('models', 'for') → ['customer']\n",
      "('for', 'customer') → ['service']\n",
      "('customer', 'service') → [',']\n",
      "('service', ',') → ['content']\n",
      "(',', 'content') → ['creation']\n",
      "('content', 'creation') → [',']\n",
      "('creation', ',') → ['and']\n",
      "('and', 'data') → ['analysis']\n",
      "('data', 'analysis') → ['.']\n",
      "('analysis', '.') → ['language']\n",
      "('.', 'language') → ['models']\n",
      "('models', 'learn') → ['patterns']\n",
      "('learn', 'patterns') → ['from']\n",
      "('patterns', 'from') → ['vast']\n",
      "('from', 'vast') → ['amounts']\n",
      "('vast', 'amounts') → ['of']\n",
      "('amounts', 'of') → ['text']\n",
      "('of', 'text') → ['data']\n",
      "('text', 'data') → ['.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text corpus\n",
    "corpus = \"\"\"Large language models are transforming how businesses operate. \n",
    "These models can understand language, generate text, and perform various tasks. \n",
    "Businesses use language models for customer service, content creation, and data analysis.\n",
    "Language models learn patterns from vast amounts of text data.\"\"\"\n",
    "\n",
    "# Build a bigram model\n",
    "bigram_model = build_ngram_model(corpus, 2)\n",
    "\n",
    "print(\"Bigram Dictionary/Model:\")\n",
    "for key, value in bigram_model.items():\n",
    "    print(f\"{key} → {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the langauge model we built to predict new words base on an input text\n",
    "\n",
    "Make sure to run the code below several times to see how the language model generates new text everytime (**A Non-Deterministic Stocastich Process**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language models are transforming how businesses operate . these models can understand language , generate text , and data analysis . language\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the model\n",
    "seed = ('language', 'models')\n",
    "generated_text = generate_text(bigram_model, seed, 20)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Language Models: Perplexity\n",
    "Perplexity measures how well a language model predicts a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 28.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def get_bigram_probability(model, unigram_counts, vocab_size, word1, word2):\n",
    "    \"\"\"Calculate the probability of a bigram.\"\"\"\n",
    "    bigram = (word1, word2)\n",
    "    bigram_count = len(model[bigram])\n",
    "    unigram_count = unigram_counts[word1]\n",
    "    # Add-one smoothing\n",
    "    probability = (bigram_count + 1) / (unigram_count + vocab_size)\n",
    "    return probability\n",
    "\n",
    "def calculate_perplexity(test_text, model, unigram_counts, vocab_size):\n",
    "    \"\"\"Calculate perplexity of test text using the bigram model.\"\"\"\n",
    "    tokens = word_tokenize(test_text.lower())\n",
    "    log_probability = 0\n",
    "    \n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        probability = get_bigram_probability(model, unigram_counts, vocab_size, tokens[i], tokens[i+1])\n",
    "        log_probability += np.log2(probability)\n",
    "    \n",
    "    # Perplexity = 2^(-average log probability)\n",
    "    perplexity = 2 ** (-log_probability / (len(tokens) - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Calculate unigram counts and vocabulary size\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "unigram_counts = Counter(tokens)\n",
    "vocab_size = len(unigram_counts)\n",
    "\n",
    "# Test the model on new text\n",
    "test_text = \"Language models help businesses understand customer feedback.\"\n",
    "perplexity = calculate_perplexity(test_text, bigram_model, unigram_counts, vocab_size)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
