{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651da710",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM API Examples\n",
    "# This script demonstrates how to interact with a LLM API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eec6c4",
   "metadata": {},
   "source": [
    "First of all, ensure all the required libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-genai streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16c584",
   "metadata": {},
   "source": [
    "Setup you API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb096fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_key = \"YOUR_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550e81d",
   "metadata": {},
   "source": [
    "#### Test API Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = genai.Client(api_key= my_api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking, making the response faster but less accurate\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a558d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a59aa9",
   "metadata": {},
   "source": [
    "#### Ask Agent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Choose a model. E.g. ‚Äúgemini-2.5-flash‚Äù or whichever you have access to. :contentReference[oaicite:1]{index=1}\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "def ask_agent(prompt: str) -> str:\n",
    "\n",
    "    # Initialize the client with your API key\n",
    "    client = genai.Client(api_key=\"\")\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=prompt\n",
    "    )\n",
    "    # The client returns a structure; here we extract the generated text\n",
    "    return response.text or \"\"\n",
    "\n",
    "def run_agent():\n",
    "    print(\"Agent ready. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.strip().lower() == \"exit\":\n",
    "            break\n",
    "        reply = ask_agent(user)\n",
    "        print(\"Agent:\", reply)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebfa92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3893b82",
   "metadata": {},
   "source": [
    "#### Simulate ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "import google.generativeai as genai\n",
    "\n",
    "# -------------------------\n",
    "# Page Configuration\n",
    "# -------------------------\n",
    "st.set_page_config(page_title=\"Gemini Chatbot with Memory\", page_icon=\"ü§ñ\")\n",
    "\n",
    "# -------------------------\n",
    "# API Key Input\n",
    "# -------------------------\n",
    "api_key = st.text_input(\"Enter your Gemini API key:\", type=\"password\")\n",
    "if not api_key:\n",
    "    st.stop()\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# -------------------------\n",
    "# Session State (Memory)\n",
    "# -------------------------\n",
    "if \"memory\" not in st.session_state:\n",
    "    st.session_state.memory = []  # stores (role, text) tuples\n",
    "\n",
    "# -------------------------\n",
    "# Sidebar Controls\n",
    "# -------------------------\n",
    "st.sidebar.header(\"‚öôÔ∏è Settings\")\n",
    "use_memory = st.sidebar.checkbox(\"Enable Memory\", value=True)\n",
    "if st.sidebar.button(\"üßπ Clear Chat\"):\n",
    "    st.session_state.memory = []\n",
    "    st.experimental_rerun()\n",
    "\n",
    "st.sidebar.markdown(\n",
    "    \"\"\"\n",
    "**Memory ON:** Model sees full chat history.  \n",
    "**Memory OFF:** Model only sees the latest prompt.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Chat UI\n",
    "# -------------------------\n",
    "st.title(\"üí¨ Gemini Chatbot (Memory Toggle Demo)\")\n",
    "st.caption(\"Demonstrating stateless vs. stateful LLM behavior using Gemini SDK\")\n",
    "\n",
    "# Display existing conversation\n",
    "for role, text in st.session_state.memory:\n",
    "    st.chat_message(role).markdown(text)\n",
    "\n",
    "# User input\n",
    "if prompt := st.chat_input(\"Type your message...\"):\n",
    "    st.chat_message(\"user\").markdown(prompt)\n",
    "    st.session_state.memory.append((\"user\", prompt))\n",
    "\n",
    "    # Build input context\n",
    "    if use_memory:\n",
    "        # Include full history\n",
    "        context = \"\\n\".join([f\"{r}: {t}\" for r, t in st.session_state.memory])\n",
    "    else:\n",
    "        # Only include latest user message\n",
    "        context = f\"user: {prompt}\"\n",
    "\n",
    "    # Generate response\n",
    "    response = model.generate_content(context)\n",
    "    reply = response.text\n",
    "\n",
    "    # Append and show\n",
    "    st.session_state.memory.append((\"assistant\", reply))\n",
    "    st.chat_message(\"assistant\").markdown(reply)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
