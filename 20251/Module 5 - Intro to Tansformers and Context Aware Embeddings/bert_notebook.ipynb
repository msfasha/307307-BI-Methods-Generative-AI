{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "<!-- ![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/images/header.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-Generative-AI/blob/main/20251/Module%205%20-%20Intro%20to%20Tansformers%20and%20Context%20Aware%20Embeddings/Context%20Aware%20Embeddings.ipynb\" target=\"_parent\">   \n",
    "   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da4bf6",
   "metadata": {},
   "source": [
    "### Context Aware Word Embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd402358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: requests in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2e2e8",
   "metadata": {},
   "source": [
    "### Display BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eefff",
   "metadata": {},
   "source": [
    "#### Use BERT to Create Context-Aware Word Embeddings\n",
    "Compare Apple company to Apple fruit and Microsoft company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21998295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'apple' (fruit) and 'orange': 0.5839\n",
      "Similarity between 'apple' (company) and 'Microsoft': 0.8549\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract contextual embedding for a word (handles subwords)\n",
    "def get_token_embedding(sentence, target_word):\n",
    "    # Tokenize the sentence and get embeddings\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get tokens and embeddings\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # Tokenize the target word the same way BERT does\n",
    "    target_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "    # Search for the position of the target word (handling subwords)\n",
    "    matches = []\n",
    "    for i in range(len(tokens) - len(target_tokens) + 1):\n",
    "        if tokens[i:i + len(target_tokens)] == target_tokens:\n",
    "            matches = list(range(i, i + len(target_tokens)))\n",
    "            break\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(f\"'{target_word}' not found in tokens: {tokens}\")\n",
    "\n",
    "    # Average the embeddings over all subword tokens\n",
    "    return embeddings[matches].mean(dim=0)\n",
    "\n",
    "# Contextual sentences\n",
    "sentence_fruit = \"He ate a fresh apple and enjoyed the fruit.\"\n",
    "sentence_company = \"Apple released a new product in the computer market.\"\n",
    "sentence_orange = \"An orange is a juicy fruit.\"\n",
    "sentence_microsoft = \"Microsoft computer was running the latest software.\"\n",
    "\n",
    "# Get embeddings\n",
    "apple_fruit = get_token_embedding(sentence_fruit, \"apple\")\n",
    "apple_company = get_token_embedding(sentence_company, \"apple\")\n",
    "orange = get_token_embedding(sentence_orange, \"orange\")\n",
    "microsoft = get_token_embedding(sentence_microsoft, \"Microsoft\")\n",
    "\n",
    "# Cosine similarity comparisons\n",
    "sim_fruit = F.cosine_similarity(apple_fruit, orange, dim=0)\n",
    "sim_company = F.cosine_similarity(apple_company, microsoft, dim=0)\n",
    "\n",
    "# Results\n",
    "print(f\"Similarity between 'apple' (fruit) and 'orange': {sim_fruit.item():.4f}\")\n",
    "print(f\"Similarity between 'apple' (company) and 'Microsoft': {sim_company.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05ecb6",
   "metadata": {},
   "source": [
    "# NLP Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008fd48",
   "metadata": {},
   "source": [
    "Basic Pipeline Usage\n",
    "1. Text Classification (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze single text\n",
    "result = classifier(\"I love using Hugging Face!\")\n",
    "print(result)\n",
    "# Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Analyze multiple texts\n",
    "texts = [\n",
    "    \"I hate this product\",\n",
    "    \"This is amazing!\",\n",
    "    \"It's okay, nothing special\"\n",
    "]\n",
    "results = classifier(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce5a1",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a34b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER pipeline\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"My name is John and I live in New York. I work at Google.\"\n",
    "entities = ner(text)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}\")\n",
    "    print(f\"Label: {entity['entity_group']}\")\n",
    "    print(f\"Score: {entity['score']:.4f}\")\n",
    "    print(f\"Start: {entity['start']}, End: {entity['end']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0c3cc",
   "metadata": {},
   "source": [
    "3. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning. \n",
    "They are especially known for their work in natural language processing. The company was founded in 2016 \n",
    "and is headquartered in New York.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"What is Hugging Face known for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890032c0",
   "metadata": {},
   "source": [
    "4. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text with custom parameters\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where robots exist,\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    for i, gen in enumerate(generated):\n",
    "        print(f\"Generation {i+1}: {gen['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb32898",
   "metadata": {},
   "source": [
    "5. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ff021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "article = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve \n",
    "from experience without being explicitly programmed. It focuses on the development of computer programs \n",
    "that can access data and use it to learn for themselves. The process of learning begins with observations \n",
    "or data, such as examples, direct experience, or instruction, in order to look for patterns in data and \n",
    "make better decisions in the future based on the examples that we provide. The primary aim is to allow \n",
    "the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Original length:\", len(article.split()))\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n",
    "print(\"Summary length:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e35a94",
   "metadata": {},
   "source": [
    "6. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I would like to order a coffee.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    translated = translator(text)\n",
    "    print(f\"English: {text}\")\n",
    "    print(f\"French: {translated[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c8e4b",
   "metadata": {},
   "source": [
    "#### Use specific model e.g. BERT to Create Questions Answering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a0246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\me\\myenv310\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\me\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a method of pre-training language representations\n",
      "Confidence: 0.6874\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries \n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering \n",
    "from transformers import pipeline \n",
    "import torch \n",
    "\n",
    "# Using pipeline (High-level API) \n",
    "qa_pipeline = pipeline( \"question-answering\",\n",
    "model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\" ) \n",
    "\n",
    "# Example usage \n",
    "context = \"\"\" BERT is a method of pre-training language representations, \n",
    "meaning that it trains a general-purpose language understanding \n",
    "model on a large text corpus (like Wikipedia), \n",
    "and then uses that model for downstream NLP tasks like question answering. \"\"\" \n",
    "\n",
    "question = \"What is BERT?\" \n",
    "result = qa_pipeline(question=question, context=context) \n",
    "print(f\"Answer: {result['answer']}\") \n",
    "print(f\"Confidence: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416b4ab",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
