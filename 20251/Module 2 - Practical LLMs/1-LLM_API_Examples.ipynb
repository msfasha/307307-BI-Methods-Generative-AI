{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d056e711",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "    <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-Generative-AI/blob/main/20251/Module%202%20-%20Practical%20LLMs/1-LLM_API_Examples.ipynb\" target=\"_blank\">   \n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"height: 25px; margin-right: 20px;\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61216d",
   "metadata": {},
   "source": [
    "## Practical LLM API Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60107266",
   "metadata": {},
   "source": [
    "#### Setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eec6c4",
   "metadata": {},
   "source": [
    "1- Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16c584",
   "metadata": {},
   "source": [
    "2- Setup you API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09628c",
   "metadata": {},
   "source": [
    "Write your key manually below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb096fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_key =\"your_api_key_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b89eda",
   "metadata": {},
   "source": [
    "3- Initiate Gemini Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f4baf",
   "metadata": {},
   "source": [
    "\n",
    "The Gemini client is a software library that allows developers to interact with the Gemini AI model provided by Google.  \n",
    "It enables users to send requests to the model for various tasks, such as generating text, answering questions, or performing sentiment analysis  \n",
    "By using the Gemini client, developers can easily integrate advanced AI capabilities into their applications without needing to understand the underlying complexities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86897c",
   "metadata": {},
   "source": [
    "Now that the environment is ready, we can run our examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d9b28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550e81d",
   "metadata": {},
   "source": [
    "#### Gemini API Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b485b",
   "metadata": {},
   "source": [
    "#### Simple Questions Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithms learn patterns from data to make predictions/decisions.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c090e6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdeea1",
   "metadata": {},
   "source": [
    "#### Summarize text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can definitely help summarize text into 3 bullet points, but the full paragraph for \"Artificial Intelligence (AI) is transforming industries...\" seems to be missing.\n",
      "\n",
      "To provide an accurate summary, I would need the complete text.\n",
      "\n",
      "Based *only* on the introductory phrase you provided, one possible bullet point (out of three) might be:\n",
      "\n",
      "*   **Artificial Intelligence (AI) is actively reshaping various industrial sectors.**\n",
      "\n",
      "Please provide the rest of the paragraph, and I'll be happy to give you a complete 3-bullet point summary!\n"
     ]
    }
   ],
   "source": [
    "# Larger sample text\n",
    "text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries across the globe by automating processes, \n",
    "enhancing decision-making, and enabling new forms of creativity. In healthcare, AI algorithms assist \n",
    "doctors in diagnosing diseases more accurately and predicting patient outcomes based on large datasets. \n",
    "In finance, AI-driven systems detect fraud, optimize investment strategies, and analyze market trends \n",
    "in real time. The education sector is also evolving, as intelligent tutoring systems provide \n",
    "personalized learning experiences that adapt to each student’s pace and learning style.\n",
    "\n",
    "Moreover, AI is revolutionizing transportation with the development of self-driving vehicles that use \n",
    "computer vision, sensor fusion, and deep learning to navigate safely. In manufacturing, predictive \n",
    "maintenance powered by AI minimizes downtime by identifying potential equipment failures before they \n",
    "occur. Despite these advancements, concerns remain regarding job displacement, data privacy, and \n",
    "ethical decision-making in autonomous systems. Balancing innovation with responsible AI governance \n",
    "is becoming a critical challenge for policymakers, researchers, and businesses alike.\n",
    "\n",
    "As AI continues to evolve, its integration with other emerging technologies—such as the Internet of \n",
    "Things (IoT), quantum computing, and blockchain—will likely amplify its impact. The future of AI holds \n",
    "tremendous promise, but realizing its benefits responsibly will require collaboration across disciplines \n",
    "and transparent communication between humans and intelligent systems.\n",
    "\"\"\"\n",
    "\n",
    "# Create a summarization prompt\n",
    "prompt = f\"Summarize the following text in 3 concise bullet points:\\n\\n{text}\"\n",
    "\n",
    "# Generate the summary\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt)\n",
    "\n",
    "# Print the summary\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87cdc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611f220",
   "metadata": {},
   "source": [
    "#### Text to code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function to calculate the factorial of a number using recursion, along with explanations and example usage.\n",
      "\n",
      "```python\n",
      "def factorial_recursive(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a non-negative integer using recursion.\n",
      "\n",
      "    The factorial of a non-negative integer n, denoted by n!, is the\n",
      "    product of all positive integers less than or equal to n.\n",
      "    The factorial of 0 is defined as 1.\n",
      "\n",
      "    Args:\n",
      "        n (int): A non-negative integer.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input is not an integer.\n",
      "        ValueError: If the input is a negative integer.\n",
      "\n",
      "    Examples:\n",
      "        >>> factorial_recursive(0)\n",
      "        1\n",
      "        >>> factorial_recursive(1)\n",
      "        1\n",
      "        >>> factorial_recursive(5)\n",
      "        120\n",
      "        >>> factorial_recursive(10)\n",
      "        3628800\n",
      "    \"\"\"\n",
      "    # --- Input Validation ---\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "\n",
      "    # --- Base Case ---\n",
      "    # The factorial of 0 is 1. This is where the recursion stops.\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    # --- Recursive Step ---\n",
      "    # n! = n * (n-1)!\n",
      "    else:\n",
      "        return n * factorial_recursive(n - 1)\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(f\"Factorial of 0: {factorial_recursive(0)}\")\n",
      "print(f\"Factorial of 1: {factorial_recursive(1)}\")\n",
      "print(f\"Factorial of 5: {factorial_recursive(5)}\")\n",
      "print(f\"Factorial of 10: {factorial_recursive(10)}\")\n",
      "\n",
      "# --- Test with invalid inputs ---\n",
      "try:\n",
      "    print(f\"Factorial of -3: {factorial_recursive(-3)}\")\n",
      "except ValueError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "\n",
      "try:\n",
      "    print(f\"Factorial of 2.5: {factorial_recursive(2.5)}\")\n",
      "except TypeError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "\n",
      "try:\n",
      "    print(f\"Factorial of 'abc': {factorial_recursive('abc')}\")\n",
      "except TypeError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1.  **Docstring:** Explains what the function does, its arguments, return value, and potential errors, along with examples. This is good practice for code readability and maintainability.\n",
      "\n",
      "2.  **Input Validation:**\n",
      "    *   `if not isinstance(n, int):`: Checks if the input `n` is an integer. If not, it raises a `TypeError`. Factorials are typically defined only for integers.\n",
      "    *   `if n < 0:`: Checks if the input `n` is negative. If so, it raises a `ValueError`. Factorials are not defined for negative numbers.\n",
      "\n",
      "3.  **Base Case (`if n == 0:`):**\n",
      "    *   This is the most crucial part of any recursive function. It defines the condition under which the recursion stops.\n",
      "    *   By definition, `0! = 1`. When `n` reaches `0`, the function simply returns `1` without making further recursive calls. This prevents infinite recursion.\n",
      "\n",
      "4.  **Recursive Step (`else: return n * factorial_recursive(n - 1)`):**\n",
      "    *   This is where the function calls itself.\n",
      "    *   The mathematical definition of factorial is `n! = n * (n-1)!`.\n",
      "    *   The function embodies this directly: it returns `n` multiplied by the factorial of `n-1`.\n",
      "    *   Each recursive call reduces the problem to a smaller version (`n-1`) until it eventually hits the base case (`n=0`).\n",
      "\n",
      "### How it works (Example `factorial_recursive(3)`):\n",
      "\n",
      "1.  `factorial_recursive(3)` is called.\n",
      "    *   `n` is 3 (not 0), so it goes to the `else` block.\n",
      "    *   It returns `3 * factorial_recursive(2)`.\n",
      "2.  `factorial_recursive(2)` is called.\n",
      "    *   `n` is 2 (not 0), so it goes to the `else` block.\n",
      "    *   It returns `2 * factorial_recursive(1)`.\n",
      "3.  `factorial_recursive(1)` is called.\n",
      "    *   `n` is 1 (not 0), so it goes to the `else` block.\n",
      "    *   It returns `1 * factorial_recursive(0)`.\n",
      "4.  `factorial_recursive(0)` is called.\n",
      "    *   `n` is 0. This is the **base case**.\n",
      "    *   It returns `1`.\n",
      "5.  Now the calls unwind:\n",
      "    *   The `factorial_recursive(1)` call receives `1` from `factorial_recursive(0)`. It calculates `1 * 1 = 1` and returns `1`.\n",
      "    *   The `factorial_recursive(2)` call receives `1` from `factorial_recursive(1)`. It calculates `2 * 1 = 2` and returns `2`.\n",
      "    *   The `factorial_recursive(3)` call receives `2` from `factorial_recursive(2)`. It calculates `3 * 2 = 6` and returns `6`.\n",
      "\n",
      "The final result `6` is returned.\n"
     ]
    }
   ],
   "source": [
    "# Define your prompt\n",
    "prompt = \"Write a Python function to calculate the factorial of a number using recursion.\"\n",
    "\n",
    "# Generate the response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt)\n",
    "\n",
    "# Print the summary\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6f7c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a59aa9",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I love this product! It's amazing.\n",
      "Sentiment: **Positive**\n",
      "\n",
      "Review: Terrible experience. I want a refund.\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: It’s okay, not great but not bad either.\n",
      "Sentiment: **Neutral**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample reviews\n",
    "reviews = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"Terrible experience. I want a refund.\",\n",
    "    \"It’s okay, not great but not bad either.\"\n",
    "]\n",
    "\n",
    "# Loop through and classify sentiment\n",
    "for r in reviews:\n",
    "    prompt = f\"Classify the sentiment of this review as Positive, Negative, or Neutral:\\n\\n{r}\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=prompt)\n",
    "    print(f\"Review: {r}\\nSentiment: {response.text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b0bcc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e379ca",
   "metadata": {},
   "source": [
    "#### Use VSCode GitHub Co Pilot OR Google Colab AI to write a Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082d6ed",
   "metadata": {},
   "source": [
    "Write code that reads the following Amazon customers Reviews file into a Pandas dataframe and display the first five rows:  \n",
    "from: https://github.com/msfasha/307307-BI-Methods-Generative-AI/blob/main/20251/Module%202%20-%20Practical%20LLMs/amazon_reviews.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322488c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a9a29d5",
   "metadata": {},
   "source": [
    "Write code that implements sentiment analysis for these reviews using Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632acc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ebfa92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0c018",
   "metadata": {},
   "source": [
    "#### LLM and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d19c0",
   "metadata": {},
   "source": [
    "Example 1 — Without Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3831ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Model reply: Hello Mohammed! It's nice to meet you. How can I help you today?\n",
      "\n",
      "Step 2 -  As an AI, I don't know your name. I don't have access to personal information about you.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tell the model your name\n",
    "response1 = client.models.generate_content(model=\"gemini-2.5-flash\", contents=\"My name is Mohammed.\")\n",
    "print(\"Step 1 - Model reply:\", response1.text)\n",
    "\n",
    "# Step 2: Ask what your name is (model has no memory between calls)\n",
    "response2 = client.models.generate_content(model=\"gemini-2.5-flash\", contents=\"What is my name?\")\n",
    "print(\"\\nStep 2 -\", response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04254a",
   "metadata": {},
   "source": [
    "Example 2 — With Memory\n",
    "\n",
    "Here, you store and reuse conversation history to simulate memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca883fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Model reply: Hello Mohammed, it's nice to meet you! How can I help you today?\n",
      "\n",
      "Step 2 - Model reply (with memory): Your name is Mohammed.\n"
     ]
    }
   ],
   "source": [
    "# Create a chat session (it remembers context automatically)\n",
    "chat = client.start_chat(history=[])\n",
    "\n",
    "def ask_with_memory(user_input):\n",
    "    # Send the message and get the model's reply\n",
    "    response = chat.send_message(user_input)\n",
    "    return response.text\n",
    "\n",
    "# Step 1: Tell the model your name\n",
    "print(\"Step 1 - Model reply:\", ask_with_memory(\"My name is Mohammed.\"))\n",
    "\n",
    "# Step 2: Ask what your name is (model now remembers)\n",
    "print(\"\\nStep 2 - Model reply (with memory):\", ask_with_memory(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b2eb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a47eb7",
   "metadata": {},
   "source": [
    "#### Mulit-Modal Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39252e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cells to set the key and create Gemini client\n",
    "# download image from https://raw.githubusercontent.com/google-research/generative-ai/main/images/car-crash.jpg\n",
    "# Upload image to colab /content folder before running this code\n",
    "\n",
    "uploaded_file = client.files.upload(file='/content/car-crash.jpg')\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "      \"اكتب تقرير يبين الاضرار في هذه المركبة\",\n",
    "      uploaded_file,\n",
    "    ]\n",
    ")\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
