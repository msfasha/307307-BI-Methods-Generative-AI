{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reasoning and Planning Frameworks in Python\n\n",
        "This notebook demonstrates four key reasoning and planning frameworks often used in intelligent agent design:\n\n",
        "1. **Chain-of-Thought (CoT)** \u2014 step-by-step reasoning  \n",
        "2. **ReAct (Reason + Act + Observe)** \u2014 reasoning combined with actions  \n",
        "3. **Self-Correction and Reflection** \u2014 self-review and improvement  \n",
        "4. **Tree of Thoughts (ToT)** \u2014 multi-path reasoning exploration  \n\n",
        "Each section includes:\n",
        "- A short conceptual overview\n",
        "- A working Python demo (simulated or real)\n",
        "- Explanations and key takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Chain-of-Thought (CoT)\n\n",
        "**Concept:**  \n",
        "Chain-of-Thought prompting encourages a model (or reasoning system) to **think step by step** before giving an answer.  \n",
        "Instead of jumping to conclusions, it explains intermediate reasoning \u2014 improving accuracy and transparency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n=========================================================\nCHAIN-OF-THOUGHT (CoT) DEMO\n---------------------------------------------------------\nThis script demonstrates step-by-step reasoning (Chain-of-Thought)\nusing either a simulated reasoning engine or an LLM API call.\n=========================================================\n\"\"\"\n\n# from openai import OpenAI\n# client = OpenAI()\n\ndef chain_of_thought_reasoning(question: str, simulate: bool = True) -> str:\n    \"\"\"\n    Demonstrates the Chain-of-Thought reasoning pattern.\n    \n    Args:\n        question (str): The problem or question to solve.\n        simulate (bool): If True, use a simulated reasoning trace.\n                         If False, query a language model.\n    \n    Returns:\n        str: The reasoning trace and final answer.\n    \"\"\"\n\n    if simulate:\n        print(\"Reasoning step by step...\\n\")\n\n        if \"train\" in question.lower() and \"speed\" in question.lower():\n            reasoning = [\n                \"Step 1: Recall that speed = distance / time.\",\n                \"Step 2: The train travels 60 km in 1.5 hours.\",\n                \"Step 3: Compute 60 / 1.5 = 40.\",\n                \"Step 4: Therefore, the average speed is 40 km/h.\"\n            ]\n            return \"\\n\".join(reasoning)\n\n        elif \"apple\" in question.lower() and \"total\" in question.lower():\n            reasoning = [\n                \"Step 1: John has 3 apples and buys 2 more.\",\n                \"Step 2: Add them together: 3 + 2 = 5.\",\n                \"Step 3: Therefore, John has 5 apples in total.\"\n            ]\n            return \"\\n\".join(reasoning)\n\n        else:\n            return \"No simulation rule defined for this question, but the reasoning would follow a step-by-step logical explanation.\"\n\n    else:\n        response = client.chat.completions.create(\n            model=\"gpt-5\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful tutor who always reasons step by step.\"},\n                {\"role\": \"user\", \"content\": f\"Question: {question}\\nLet's reason step by step before answering.\"}\n            ],\n            temperature=0.3\n        )\n        return response.choices[0].message.content\n\n\n# Example Run\nquestion = \"A train travels 60 km in 1.5 hours. What is its average speed?\"\nprint(chain_of_thought_reasoning(question, simulate=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ReAct (Reason + Act + Observe)\n\n",
        "**Concept:**  \n",
        "ReAct combines reasoning with *actions* and *observations* in a feedback loop.  \n",
        "This allows an agent to think, perform external actions (like searches), observe the results, and reason again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n=========================================================\nREACT (Reasoning + Acting + Observing) DEMO\n---------------------------------------------------------\nThis script demonstrates a simplified version of the ReAct\nframework, where an agent interleaves reasoning steps with\nactions and observations.\n=========================================================\n\"\"\"\n\nimport time\n\nKNOWLEDGE_BASE = {\n    \"population of paris\": \"The population of Paris is approximately 2.1 million (as of 2025).\",\n    \"capital of france\": \"The capital of France is Paris.\",\n    \"height of eiffel tower\": \"The Eiffel Tower is about 330 meters tall.\"\n}\n\ndef search_knowledge_base(query: str) -> str:\n    for key, value in KNOWLEDGE_BASE.items():\n        if key in query.lower():\n            return value\n    return \"No results found.\"\n\ndef react_agent(question: str, max_steps: int = 3) -> str:\n    print(f\"Question: {question}\\n\")\n    observation = \"\"\n\n    for step in range(1, max_steps + 1):\n        print(f\"--- Step {step} ---\")\n\n        if step == 1:\n            thought = f\"I need to find information about '{question.lower()}'.\"\n        else:\n            thought = f\"Based on what I observed, I will check if I now have the answer.\"\n        print(f\"Thought: {thought}\")\n\n        if \"find\" in thought.lower():\n            action = f\"Search knowledge base for: {question}\"\n            print(f\"Action: {action}\")\n            observation = search_knowledge_base(question)\n        else:\n            action = \"Review observations and summarize.\"\n            print(f\"Action: {action}\")\n\n        print(f\"Observation: {observation}\")\n\n        if \"No results\" not in observation:\n            print(\"Final Answer Found.\")\n            return f\"Answer: {observation}\\n\"\n\n        print(\"No conclusive result, reasoning further...\\n\")\n        time.sleep(0.3)\n\n    return \"Answer: Unable to find sufficient information.\"\n\n# Example Run\nquestion = \"What is the population of Paris?\"\nprint(react_agent(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Self-Correction and Reflection\n\n",
        "**Concept:**  \n",
        "This framework introduces *self-review*:  \n",
        "1. The agent produces an initial answer.  \n",
        "2. It critiques its own answer.  \n",
        "3. It revises and improves it.\n\n",
        "This mirrors how humans reflect and refine their work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n=========================================================\nSELF-CORRECTION AND REFLECTION DEMO\n---------------------------------------------------------\nThis script demonstrates a reasoning pattern where an agent:\n1. Generates an initial answer.\n2. Reflects on and critiques that answer.\n3. Produces a revised, improved version.\n=========================================================\n\"\"\"\n\ndef self_correct(question: str, simulate: bool = True) -> str:\n    if simulate:\n        print(f\"Question: {question}\\n\")\n        initial_answer = \"Isaac Newton discovered gravity in the 1600s.\"\n        print(f\"Initial Answer: {initial_answer}\")\n\n        critique = (\n            \"Critique: Gravity was not discovered by Newton; \"\n            \"he formulated the law of universal gravitation. \"\n            \"Objects always experienced gravity, but Newton mathematically described it.\"\n        )\n        print(critique)\n\n        improved_answer = (\n            \"Corrected Answer: Isaac Newton did not 'discover' gravity; \"\n            \"he formulated the law of universal gravitation in the late 17th century, \"\n            \"explaining how every mass attracts every other mass.\"\n        )\n\n        print(f\"\\nFinal Answer: {improved_answer}\")\n        return improved_answer\n\n# Example Run\nquestion = \"Who discovered gravity?\"\nself_correct(question, simulate=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tree of Thoughts (ToT)\n\n",
        "**Concept:**  \n",
        "Tree of Thoughts generalizes Chain-of-Thought into a *search process*:\n- Multiple reasoning paths are explored in parallel.\n- Each path is evaluated and scored.\n- The best path is chosen as the final reasoning sequence.\n\n",
        "This approach is ideal for complex reasoning, puzzles, or planning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n=========================================================\nTREE OF THOUGHTS (ToT) DEMO\n---------------------------------------------------------\nThis script demonstrates how to explore multiple reasoning\npaths (\"thoughts\"), evaluate them, and select the best path.\n=========================================================\n\"\"\"\n\ndef generate_branches(thought_path: list, problem: str) -> list:\n    last_thought = thought_path[-1] if thought_path else \"Start\"\n\n    if \"raining\" in problem.lower():\n        return [\n            thought_path + [\"If it's raining, the ground must be wet. The ground is wet, so it's raining.\"],\n            thought_path + [\"The ground is wet, but that doesn't necessarily mean it\u2019s raining (there could be other causes).\"],\n            thought_path + [\"Perhaps it's raining because the ground is wet \u2014 but that reasoning is circular.\"]\n        ]\n    else:\n        return [thought_path + [f\"Continue reasoning from: {last_thought}\"]]\n\ndef evaluate_reasoning_path(thought_path: list) -> float:\n    text = \" \".join(thought_path).lower()\n    if \"doesn't necessarily\" in text or \"not necessarily\" in text:\n        return 0.9\n    elif \"so it's raining\" in text:\n        return 0.4\n    else:\n        return 0.6\n\ndef summarize_best_path(thought_path: list) -> str:\n    return \" \".join(thought_path)\n\ndef tree_of_thoughts(problem: str, depth: int = 2, breadth: int = 3) -> str:\n    print(f\"Problem: {problem}\\n\")\n    thoughts = [{\"path\": [\"Begin reasoning\"], \"score\": 0.0}]\n\n    for level in range(depth):\n        print(f\"--- Level {level + 1} ---\")\n        new_thoughts = []\n\n        for t in thoughts:\n            branches = generate_branches(t[\"path\"], problem)\n            for b in branches:\n                score = evaluate_reasoning_path(b)\n                new_thoughts.append({\"path\": b, \"score\": score})\n\n        thoughts = sorted(new_thoughts, key=lambda x: x[\"score\"], reverse=True)[:breadth]\n\n        for i, t in enumerate(thoughts, 1):\n            print(f\"Candidate {i}: Score={t['score']:.2f} | Path={t['path'][-1]}\")\n\n    best = thoughts[0]\n    print(\"\\nChosen Path:\\n\" + summarize_best_path(best[\"path\"]))\n    return summarize_best_path(best[\"path\"])\n\n# Example Run\nproblem = \"If it's raining, the ground is wet. The ground is wet. Does that mean it's raining?\"\ntree_of_thoughts(problem)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}