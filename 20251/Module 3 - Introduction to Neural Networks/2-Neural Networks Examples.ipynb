{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "<!-- ![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/images/header.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-Generative-AI/blob/main/20251/Module%203%20-%20Introduction%20to%20Neural%20Networks/2-Neural%20Networks%20Examples.ipynb\" target=\"_parent\"><img    \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63640f64",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d301173",
   "metadata": {},
   "source": [
    "### Implement the Perceptron Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Parameters\n",
    "w = np.array([0.1, 0.3])   # initial weights\n",
    "b = 0.2                    # initial bias\n",
    "eta = 0.1                  # learning rate\n",
    "\n",
    "# Perceptron training\n",
    "for epoch in range(100):  # limit to prevent infinite loops\n",
    "    total_error = 0\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        x1, x2 = X[i]\n",
    "        target = y[i]\n",
    "        \n",
    "        # Weighted sum\n",
    "        z = w[0]*x1 + w[1]*x2 + b\n",
    "        output = 1 if z > 0 else 0\n",
    "        error = target - output\n",
    "        \n",
    "        # Update rule\n",
    "        w[0] = w[0] + eta * error * x1\n",
    "        w[1] = w[1] + eta * error * x2\n",
    "        b = b + eta * error\n",
    "        \n",
    "        total_error += abs(error)\n",
    "        \n",
    "        # Print details for each record\n",
    "        print(f\"Record {i+1}: x1={x1}, x2={x2}, y={target}\")\n",
    "        print(f\"Weighted sum (z) = {z:.2f}, Output = {output}, Error = {error}\")\n",
    "        print(f\"Updated weights: w1={w[0]:.2f}, w2={w[1]:.2f}, b={b:.2f}\\n\")\n",
    "\n",
    "    # Stop when no errors\n",
    "    if total_error == 0:\n",
    "        print(\"Training complete â€” no errors remaining.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFinal weights:\")\n",
    "print(f\"w1 = {w[0]:.2f}, w2 = {w[1]:.2f}, b = {b:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93aa7e7",
   "metadata": {},
   "source": [
    "### Implement the Perceptron using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59862a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[0.2 0.2]]\n",
      "Bias: [-0.2]\n",
      "Predictions: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "# Training data for AND gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Initialize and train Perceptron\n",
    "model = Perceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Weights:\", model.coef_)\n",
    "print(\"Bias:\", model.intercept_)\n",
    "print(\"Predictions:\", model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b934bb",
   "metadata": {},
   "source": [
    "Note: In the scikit-learn Perceptron, the step function (also called the activation function) is a hard threshold function, and it's built-in.<br>\n",
    "\n",
    "```Python\n",
    "prediction = 1 if output >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d83e5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-Generative-AI/main/images/mlp.png\" alt=\"Simple Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c4989",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66abb8",
   "metadata": {},
   "source": [
    "## Solving the XOR Problem using a Mulit-Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3dcd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [0 1 1 0]\n",
      "\n",
      "Weights (input to hidden):\n",
      " [ w11 , w12 ]\n",
      "[ w21 , w22 ]\n",
      " [[ 2.7144501   3.27401218]\n",
      " [-2.73418453 -3.17014048]]\n",
      "\n",
      "Bias hidden:\n",
      " [ 1.21994174 -1.63451199]\n",
      "\n",
      "Weights (hidden to output):\n",
      " [[-4.37775211]\n",
      " [ 4.46553876]]\n",
      "\n",
      "Bias output:\n",
      " [3.61855675]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# XOR input and output\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Define MLP with 1 hidden layer of 2 neurons (minimal config for XOR)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,), activation='tanh',\n",
    "                    solver='adam', learning_rate_init=0.01,\n",
    "                    max_iter=10000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X)\n",
    "\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "print(\"\\nWeights (input to hidden):\\n\", \"[ w11 , w12 ]\\n[ w21 , w22 ]\\n\", mlp.coefs_[0])\n",
    "print(\"\\nBias hidden:\\n\", mlp.intercepts_[0])\n",
    "print(\"\\nWeights (hidden to output):\\n\", mlp.coefs_[1])\n",
    "print(\"\\nBias output:\\n\", mlp.intercepts_[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c2f18",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-Generative-AI/main/images/mlp.png\" alt=\"Multi Layer Perceptron\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b030aab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822617d",
   "metadata": {},
   "source": [
    "## Training a Neural Network to Classify MNIST Dataset\n",
    "We will use the **Keras library** to easily train a neural network to classify MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f3d88",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/msfasha/307307-BI-Methods-Generative-AI/main/images/mnist.png\" alt=\"MNIST Dataset\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6a6dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Load example data (MNIST digits)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1. Load example data (MNIST digits)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize to [0,1]\n",
    "\n",
    "# 2. Build a simple neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  # flatten 28x28 images\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compile and train\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"Evaluating the Model\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee06af",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
