{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc54330",
   "metadata": {},
   "source": [
    "<!-- ![Alt Text](https://raw.githubusercontent.com/msfasha/307304-Data-Mining/main/images/header.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3fa2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center;\">\n",
    "   <a href=\"https://colab.research.google.com/github/msfasha/307307-BI-Methods-Generative-AI/blob/main/20243/Part%202%20-%20Introduction%20to%20NNs%20and%20Word%20Embeddings/Introduction%20to%20Neural%20Networks%20and%20Word%20Embeddings-Python.ipynb\" target=\"_parent\"><img \n",
    "   src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedec251",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908610a",
   "metadata": {},
   "source": [
    "### Building Word Embeddings from Scratch\n",
    "We can build word embeddings from sratch using a corpus of our own and using gensim library to build Word2Vec representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0455fb",
   "metadata": {},
   "source": [
    "First, we need to install gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code can run on local machine\n",
    "#%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053491b",
   "metadata": {},
   "source": [
    "> If faced with compatibility issues in Google Colab, run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a017eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to fix the issue with gensim version on Google Colab\n",
    "%pip uninstall -y gensim\n",
    "%pip install --force-reinstall \"scipy<1.11\" \"gensim==4.3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273645e",
   "metadata": {},
   "source": [
    "We also need to install nltk to tokenize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\me\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\me\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector for 'business' (first 10 dimensions):\n",
      "[ 0.00816812 -0.00444303  0.00898543  0.00825366 -0.00443522  0.00030311\n",
      "  0.00427449 -0.00392632 -0.00555997 -0.00651232]\n",
      "Words most similar to 'language':\n",
      "natural: 0.2196\n",
      "between: 0.2167\n",
      "resources: 0.1955\n",
      "distributed: 0.1696\n",
      "significant: 0.1522\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Large language models are transforming business applications\",\n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"Word embeddings capture semantic relationships between words\",\n",
    "    \"Neural networks learn distributed representations of words\",\n",
    "    \"Businesses use language models for various applications\",\n",
    "    \"Customer service can be improved with language technology\",\n",
    "    \"Modern language models require significant computing resources\",\n",
    "    \"Language models can generate human-like text for businesses\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,    # Embedding dimension\n",
    "    window=5,           # Context window size\n",
    "    min_count=1,        # Minimum word frequency\n",
    "    workers=4           # Number of threads\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3f5d8",
   "metadata": {},
   "source": [
    "#### Display vector for a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector for 'business' (first 10 dimensions):\n",
      "[ 0.00816812 -0.00444303  0.00898543  0.00825366 -0.00443522  0.00030311\n",
      "  0.00427449 -0.00392632 -0.00555997 -0.00651232]\n"
     ]
    }
   ],
   "source": [
    "word_vector = model.wv[\"business\"]\n",
    "print(f\"\\nVector for 'business' (first 10 dimensions):\\n{word_vector[:10]}\") # Print first 10 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2d812",
   "metadata": {},
   "source": [
    "#### Find the most similar words to \"language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea26e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'language':\n",
      "natural: 0.2196\n",
      "between: 0.2167\n",
      "resources: 0.1955\n",
      "distributed: 0.1696\n",
      "significant: 0.1522\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(\"language\", topn=5)\n",
    "print(\"Words most similar to 'language':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39a93a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470336fa",
   "metadata": {},
   "source": [
    "#### Use Real Embeddings - Gensim library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50fda4",
   "metadata": {},
   "source": [
    "For this example, we will use the `word2vec-google-news-300` model. <br>\n",
    "The `word2vec-google-news-300` model is a **pre-trained Word2Vec model** created by Google.  \n",
    "It has the following characteristics:\n",
    "\n",
    "- **Training data**: Trained on approximately **100 billion words** from the **Google News** dataset.\n",
    "- **Vector size**: Each word is represented as a **300-dimensional** vector.\n",
    "- **Vocabulary size**: It contains **about 3 million unique words and phrases**.\n",
    "- **Training method**: It uses the **skip-gram** Word2Vec architecture to predict context words given a target word.\n",
    "- It's a little **heavy**, about 1.5 GB, so itâ€™s good to know the best way to handle it.\n",
    "\n",
    "This model captures a wide range of **semantic** and **syntactic** relationships between words.  \n",
    "Because it is trained on a large and diverse corpus, it is widely used for many natural language processing (NLP) tasks where high-quality word embeddings are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2dcc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'computer':\n",
      "computers: 0.7979\n",
      "laptop: 0.6640\n",
      "laptop_computer: 0.6549\n",
      "Computer: 0.6473\n",
      "com_puter: 0.6082\n",
      "\n",
      "Result of analogy (king - man + woman):\n",
      "queen: 0.7118\n",
      "\n",
      "Odd one out in ['breakfast', 'lunch', 'dinner', 'car']: car\n",
      "\n",
      "Similarity between 'coffee' and 'tea': 0.5635\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Find words similar to 'computer'\n",
    "similar_words = word2vec_model.most_similar('computer', topn=5)\n",
    "print(\"Words similar to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "\n",
    "# Example of a word analogy: king - man + woman = ?\n",
    "analogy_result = word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"\\nResult of analogy (king - man + woman):\")\n",
    "for word, similarity in analogy_result:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "\n",
    "# Find odd one out\n",
    "odd_one_out = word2vec_model.doesnt_match([\"breakfast\", \"lunch\", \"dinner\", \"car\"])\n",
    "print(\"\\nOdd one out in ['breakfast', 'lunch', 'dinner', 'car']:\", odd_one_out)\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity_score = word2vec_model.similarity('coffee', 'tea')\n",
    "print(f\"\\nSimilarity between 'coffee' and 'tea': {similarity_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
